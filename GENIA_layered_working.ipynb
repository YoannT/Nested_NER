{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlstruct.text import huggingface_tokenize, regex_sentencize, partition_spans, encode_as_tag, split_into_spans, apply_substitutions, apply_deltas\n",
    "from nlstruct.dataloaders import load_from_brat, load_genia_ner\n",
    "from nlstruct.collections import Dataset, Batcher\n",
    "from nlstruct.utils import merge_with_spans, normalize_vocabularies, factorize_rows, df_to_csr, factorize, torch_global as tg\n",
    "from nlstruct.modules.crf import BIODecoder, BIOULDecoder\n",
    "from nlstruct.environment import root, cached\n",
    "from nlstruct.train import seed_all\n",
    "from itertools import chain, repeat\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To debug the training, we can just comment the \"def run_epoch()\" and execute the function body manually without changing anything to it\n",
    "def extract_mentions(batcher, all_nets, max_depth=10):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    batcher: Batcher \n",
    "        The batcher containing the text from which we want to extract the mentions (and maybe the gold mentions)\n",
    "    ner_net: torch.nn.Module\n",
    "    max_depth: int\n",
    "        Max number of times we run the model per sample\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Batcher\n",
    "    \"\"\"\n",
    "    pred_batches = []\n",
    "    n_mentions = 0\n",
    "    ner_net = all_nets[\"ner_net\"]\n",
    "    with evaluating(all_nets):\n",
    "        with torch.no_grad():\n",
    "            for batch_i, batch in enumerate(batcher['sentence'].dataloader(batch_size=batch_size, shuffle=False, sparse_sort_on=\"token_mask\", device=tg.device)):\n",
    "\n",
    "                mask = batch[\"token_mask\"]\n",
    "\n",
    "                res = all_nets[\"ner_net\"].forward(\n",
    "                    tokens =         batch[\"token\"],\n",
    "                    mask =           mask,\n",
    "                    return_mentions=True,\n",
    "                    return_loss=False,\n",
    "                )\n",
    "                \n",
    "                pred_batch = Batcher({\n",
    "                    \"mention\": {\n",
    "                        \"mention_id\": torch.arange(n_mentions, n_mentions+len(res['doc_id']), device=device),\n",
    "                        \"begin\": res[\"begin\"],\n",
    "                        \"end\": res[\"end\"],\n",
    "                        \"ner_label\": res[\"ner_label\"],\n",
    "                        \"@sentence_id\": res[\"doc_id\"],\n",
    "                    },\n",
    "                    \"sentence\": dict(batch[\"sentence\", [\"sentence_id\", \"doc_id\"]]),\n",
    "                    \"doc\": dict(batch[\"doc\"])}, \n",
    "                    check=False)\n",
    "                \n",
    "                pred_batches.append(pred_batch)\n",
    "                print(len(res[\"begin\"]), \" mentions predicted\")\n",
    "    return Batcher.concat(pred_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Define the training metrics\n",
    "metrics_info = defaultdict(lambda: False)\n",
    "flt_format = (5, \"{:.4f}\".format)\n",
    "metrics_info.update({\n",
    "    \"train_loss\": {\"goal\": 0, \"format\": flt_format},\n",
    "    \"train_ner_loss\": {\"goal\": 0, \"format\": flt_format},\n",
    "    #\"train_recall\": {\"goal\": 1, \"format\": flt_format, \"name\": \"train_rec\"},\n",
    "    #\"train_precision\": {\"goal\": 1, \"format\": flt_format, \"name\": \"train_prec\"},\n",
    "    \"train_f1\": {\"goal\": 1, \"format\": flt_format, \"name\": \"train_f1\"},\n",
    "    \n",
    "    \"val_loss\": {\"goal\": 0, \"format\": flt_format},\n",
    "    \"val_ner_loss\": {\"goal\": 0, \"format\": flt_format},\n",
    "    \"val_label_loss\": {\"goal\": 0, \"format\": flt_format},\n",
    "    \n",
    "    \"val_f1\": {\"goal\": 1, \"format\": flt_format, \"name\": \"val_f1\"},\n",
    "    \"val_3.1_f1\": {\"goal\": 1, \"format\": flt_format, \"name\": \"val_3.1_f1\"},\n",
    "    \"val_3.2_f1\": {\"goal\": 1, \"format\": flt_format, \"name\": \"val_3.2_f1\"},\n",
    "    \"val_macro_f1\": {\"goal\": 1, \"format\": flt_format, \"name\": \"val_macro_f1\"},\n",
    "    \"val_sosy_f1\": {\"goal\": 1, \"format\": flt_format, \"name\": \"val_sosy_f1\"},\n",
    "    \"val_pathologie_f1\": {\"goal\": 1, \"format\": flt_format, \"name\": \"val_patho_f1\"},\n",
    "    \n",
    "    \"duration\": {\"format\": flt_format, \"name\": \"   dur(s)\"},\n",
    "    \"rescale\": {\"format\": flt_format},\n",
    "    \"n_depth\": {\"format\": flt_format},\n",
    "    \"n_matched\": {\"format\": flt_format},\n",
    "    \"n_targets\": {\"format\": flt_format},\n",
    "    \"n_observed\": {\"format\": flt_format},\n",
    "    \"total_score_sum\": {\"format\": flt_format},\n",
    "    \"lr\": {\"format\": (5, \"{:.2e}\".format)},\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlstruct.utils import encode_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@cached\n",
    "def preprocess_train(\n",
    "    dataset,\n",
    "    max_sentence_length,\n",
    "    bert_name,\n",
    "    ner_labels=None,\n",
    "    unknown_labels=\"drop\",\n",
    "    vocabularies=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "        dataset: Dataset\n",
    "        max_sentence_length: int\n",
    "            Max number of \"words\" as defined by the regex in regex_sentencize (so this is not the nb of wordpieces)\n",
    "        bert_name: str\n",
    "            bert path/name\n",
    "        ner_labels: list of str \n",
    "            allowed ner labels (to be dropped or filtered)\n",
    "        unknown_labels: str\n",
    "            \"drop\" or \"raise\"\n",
    "        vocabularies: dict[str; np.ndarray or list]\n",
    "    Returns\n",
    "    -------\n",
    "    (pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, dict[str; np.ndarray or list])\n",
    "        docs:      ('split', 'text', 'doc_id')\n",
    "        sentences: ('split', 'doc_id', 'sentence_idx', 'begin', 'end', 'text', 'sentence_id')\n",
    "        mentions:  ('ner_label', 'doc_id', 'sentence_id', 'mention_id', 'depth', 'text', 'mention_idx', 'begin', 'end')\n",
    "        tokens:    ('split', 'token', 'sentence_id', 'token_id', 'token_idx', 'begin', 'end', 'doc_id', 'sentence_idx')\n",
    "        deltas:    ('doc_id', 'begin', 'end', 'delta')\n",
    "        vocs: vocabularies to be reused later for encoding more data or decoding predictions\n",
    "    \"\"\"\n",
    "    print(\"Dataset:\", dataset)\n",
    "    mentions = dataset[\"mentions\"].rename({\"label\": \"ner_label\"}, axis=1)\n",
    "    if ner_labels is not None:\n",
    "        len_before = len(mentions)\n",
    "        unknown_ner_labels = list(mentions[~mentions[\"ner_label\"].isin(ner_labels)][\"ner_label\"].drop_duplicates())\n",
    "        mentions = mentions[mentions[\"ner_label\"].isin(ner_labels)]\n",
    "        if len(unknown_ner_labels) and unknown_labels == \"raise\":\n",
    "            raise Exception(f\"Unkown labels in {len_before-len(mentions)} mentions: \", unknown_ner_labels)\n",
    "    # Check that there is no mention overlap\n",
    "    mentions = mentions.merge(dataset[\"fragments\"].groupby([\"doc_id\", \"mention_id\"], as_index=False, observed=True).agg({\"begin\": \"min\", \"end\": \"max\"}))\n",
    "    print(\"Transform texts...\", end=\" \")\n",
    "    docs, deltas = apply_substitutions(\n",
    "        dataset[\"docs\"], *zip(\n",
    "            #(r\"(?<=[{}\\\\])(?![ ])\".format(string.punctuation), r\" \"),\n",
    "            #(r\"(?<![ ])(?=[{}\\\\])\".format(string.punctuation), r\" \"),\n",
    "            (\"(?<=[a-zA-Z])(?=[0-9])\", r\" \"),\n",
    "            (\"(?<=[0-9])(?=[A-Za-z])\", r\" \"),\n",
    "        ), apply_unidecode=True)\n",
    "    docs = docs.astype({\"text\": str})\n",
    "    transformed_mentions = apply_deltas(mentions, deltas, on=['doc_id'])\n",
    "    print(\"done\")\n",
    "    \n",
    "    print(\"Splitting into sentences...\", end=\" \")\n",
    "    sentences = regex_sentencize(\n",
    "        docs, \n",
    "        reg_split=r\"(?<=[.])(\\s*\\n+)|(?=, [0-9]\\))\",\n",
    "        min_sentence_length=None, max_sentence_length=max_sentence_length,\n",
    "        balance_parentheses=False,\n",
    "        # balance_parentheses=True, # default is True\n",
    "    )\n",
    "    [mentions], sentences, sentence_to_docs = partition_spans([transformed_mentions], sentences, new_id_name=\"sentence_id\", overlap_policy=False)\n",
    "    n_sentences_per_mention = mentions.assign(count=1).groupby([\"doc_id\", \"mention_id\"], as_index=False).agg({\"count\": \"sum\", \"text\": tuple, \"sentence_id\": tuple})\n",
    "    if n_sentences_per_mention[\"count\"].max() > 1:\n",
    "        display(n_sentences_per_mention.query(\"count > 1\"))\n",
    "        display(sentences[sentences[\"sentence_id\"].isin(n_sentences_per_mention.query(\"count > 1\")[\"sentence_id\"].explode())][\"text\"].tolist())\n",
    "        raise Exception(\"Some mentions could be mapped to more than 1 sentences ({})\".format(n_sentences_per_mention[\"count\"].max()))\n",
    "    if sentence_to_docs is not None:\n",
    "        mentions = mentions.merge(sentence_to_docs)\n",
    "    mentions = mentions.assign(mention_idx=0).nlstruct.groupby_assign([\"doc_id\", \"sentence_id\"], {\"mention_idx\": lambda x: tuple(range(len(x)))})\n",
    "    print(\"done\")\n",
    "    \n",
    "    print(\"Tokenizing...\", end=\" \")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(bert_name)\n",
    "    sentences[\"text\"] = sentences[\"text\"].str.lower()\n",
    "    tokens = huggingface_tokenize(sentences, tokenizer, doc_id_col=\"sentence_id\")\n",
    "    \n",
    "    print(tokens)\n",
    "    \n",
    "    mentions = split_into_spans(mentions, tokens, pos_col=\"token_idx\", overlap_policy=False)\n",
    "    print(\"done\")\n",
    "    \n",
    "    print(\"Processing nestings (overlapping areas)...\", end=\" \")\n",
    "    # Extract overlapping spans\n",
    "    conflicts = (\n",
    "        merge_with_spans(mentions, mentions, on=[\"doc_id\", \"sentence_id\", (\"begin\", \"end\")], how=\"outer\", suffixes=(\"\", \"_other\"))\n",
    "    )\n",
    "    # ids1, and ids2 make the edges of the overlapping mentions of the same type (see the \"ner_label\")\n",
    "    [ids1, ids2], unique_ids = factorize_rows(\n",
    "        [conflicts[[\"doc_id\", \"sentence_id\", \"mention_id\"]], \n",
    "         conflicts[[\"doc_id\", \"sentence_id\", \"mention_id_other\"]]],\n",
    "        mentions.eval(\"size=(end-begin)\").sort_values(\"size\")[[\"doc_id\", \"sentence_id\", \"mention_id\"]]\n",
    "    )\n",
    "    g = nx.from_scipy_sparse_matrix(df_to_csr(ids1, ids2, n_rows=len(unique_ids), n_cols=len(unique_ids)))\n",
    "    colored_nodes = np.asarray(list(nx.coloring.greedy_color(g, strategy=keep_order).items()))\n",
    "    unique_ids['depth'] = colored_nodes[:, 1][colored_nodes[:, 0].argsort()]\n",
    "    mentions = mentions.merge(unique_ids)\n",
    "    print(\"done\")\n",
    "    \n",
    "    print(\"Computing vocabularies...\")\n",
    "    [docs, sentences, mentions, tokens], vocs = normalize_vocabularies(\n",
    "        [docs, sentences, mentions, tokens], \n",
    "        vocabularies={\"split\": [\"train\", \"val\", \"test\"]} if vocabularies is None else vocabularies,\n",
    "        train_vocabularies={\"source\": False, \"text\": False} if vocabularies is None else False,\n",
    "        verbose=True)\n",
    "    print(\"done\")\n",
    "    \n",
    "    prep = Dataset(docs=docs, sentences=sentences, mentions=mentions, tokens=tokens).copy()\n",
    "    \n",
    "    unique_mention_ids = encode_ids([mentions], (\"doc_id\", \"sentence_id\", \"mention_id\"))\n",
    "    unique_sentence_ids = encode_ids([sentences, mentions, tokens], (\"doc_id\", \"sentence_id\"))\n",
    "    unique_doc_ids = encode_ids([docs, sentences, mentions, tokens], (\"doc_id\",))\n",
    "    \n",
    "    batcher = Batcher({\n",
    "        \"mention\": {\n",
    "            \"mention_id\": mentions[\"mention_id\"],\n",
    "            \"sentence_id\": mentions[\"sentence_id\"],\n",
    "            \"doc_id\": mentions[\"doc_id\"],\n",
    "            \"begin\": mentions[\"begin\"],\n",
    "            \"end\": mentions[\"end\"],\n",
    "            \"depth\": mentions[\"depth\"],\n",
    "            \"ner_label\": mentions[\"ner_label\"].cat.codes,\n",
    "        },\n",
    "        \"sentence\": {\n",
    "            \"sentence_id\": sentences[\"sentence_id\"],\n",
    "            \"doc_id\": sentences[\"doc_id\"],\n",
    "            \"mention_id\": df_to_csr(mentions[\"sentence_id\"], mentions[\"mention_idx\"], mentions[\"mention_id\"], n_rows=len(unique_sentence_ids)),\n",
    "            \"mention_mask\": df_to_csr(mentions[\"sentence_id\"], mentions[\"mention_idx\"], n_rows=len(unique_sentence_ids)),\n",
    "            \"token\": df_to_csr(tokens[\"sentence_id\"], tokens[\"token_idx\"], tokens[\"token\"].cat.codes, n_rows=len(unique_sentence_ids)),\n",
    "            \"token_mask\": df_to_csr(tokens[\"sentence_id\"], tokens[\"token_idx\"], n_rows=len(unique_sentence_ids)),\n",
    "        },\n",
    "        \"doc\": {\n",
    "            \"doc_id\": np.arange(len(unique_doc_ids)),\n",
    "            \"sentence_id\": df_to_csr(sentences[\"doc_id\"], sentences[\"sentence_idx\"], sentences[\"sentence_id\"], n_rows=len(unique_doc_ids)),\n",
    "            \"sentence_mask\": df_to_csr(sentences[\"doc_id\"], sentences[\"sentence_idx\"], n_rows=len(unique_doc_ids)),\n",
    "            \"split\": docs[\"split\"].cat.codes,\n",
    "        }},\n",
    "        masks={\"sentence\": {\"token\": \"token_mask\", \"mention_id\": \"mention_mask\"}, \n",
    "               \"doc\": {\"sentence_id\": \"sentence_mask\"}}\n",
    "    )\n",
    "    \n",
    "    return batcher, prep, deltas, vocs\n",
    "\n",
    "def keep_order(G, colors):\n",
    "    \"\"\"Returns a list of the nodes of ``G`` in ordered identically to their id in the graph\n",
    "    ``G`` is a NetworkX graph. ``colors`` is ignored.\n",
    "    This is to assign a depth using the nx.coloring.greedy_color function\n",
    "    \"\"\"\n",
    "    return sorted(list(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>token</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token_idx</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>sentence_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>[CLS]</td>\n",
       "      <td>MEDLINE:95369245/0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MEDLINE:95369245</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train</td>\n",
       "      <td>il</td>\n",
       "      <td>MEDLINE:95369245/0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>MEDLINE:95369245</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>-</td>\n",
       "      <td>MEDLINE:95369245/0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>MEDLINE:95369245</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>MEDLINE:95369245/0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>MEDLINE:95369245</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>gene</td>\n",
       "      <td>MEDLINE:95369245/0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>MEDLINE:95369245</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798171</th>\n",
       "      <td>test</td>\n",
       "      <td>2</td>\n",
       "      <td>MEDLINE:95370270/7</td>\n",
       "      <td>798171</td>\n",
       "      <td>87</td>\n",
       "      <td>281</td>\n",
       "      <td>282</td>\n",
       "      <td>MEDLINE:95370270</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798172</th>\n",
       "      <td>test</td>\n",
       "      <td>secret</td>\n",
       "      <td>MEDLINE:95370270/7</td>\n",
       "      <td>798172</td>\n",
       "      <td>88</td>\n",
       "      <td>283</td>\n",
       "      <td>289</td>\n",
       "      <td>MEDLINE:95370270</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798173</th>\n",
       "      <td>test</td>\n",
       "      <td>##ion</td>\n",
       "      <td>MEDLINE:95370270/7</td>\n",
       "      <td>798173</td>\n",
       "      <td>89</td>\n",
       "      <td>289</td>\n",
       "      <td>292</td>\n",
       "      <td>MEDLINE:95370270</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798174</th>\n",
       "      <td>test</td>\n",
       "      <td>.</td>\n",
       "      <td>MEDLINE:95370270/7</td>\n",
       "      <td>798174</td>\n",
       "      <td>90</td>\n",
       "      <td>292</td>\n",
       "      <td>293</td>\n",
       "      <td>MEDLINE:95370270</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798175</th>\n",
       "      <td>test</td>\n",
       "      <td>[SEP]</td>\n",
       "      <td>MEDLINE:95370270/7</td>\n",
       "      <td>798175</td>\n",
       "      <td>91</td>\n",
       "      <td>293</td>\n",
       "      <td>293</td>\n",
       "      <td>MEDLINE:95370270</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>798176 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        split   token         sentence_id  token_id  token_idx  begin  end            doc_id  sentence_idx\n",
       "0       train   [CLS]  MEDLINE:95369245/0         0          0      0    0  MEDLINE:95369245             0\n",
       "1       train      il  MEDLINE:95369245/0         1          1      0    2  MEDLINE:95369245             0\n",
       "2       train       -  MEDLINE:95369245/0         2          2      2    3  MEDLINE:95369245             0\n",
       "3       train       2  MEDLINE:95369245/0         3          3      3    4  MEDLINE:95369245             0\n",
       "4       train    gene  MEDLINE:95369245/0         4          4      5    9  MEDLINE:95369245             0\n",
       "...       ...     ...                 ...       ...        ...    ...  ...               ...           ...\n",
       "798171   test       2  MEDLINE:95370270/7    798171         87    281  282  MEDLINE:95370270             7\n",
       "798172   test  secret  MEDLINE:95370270/7    798172         88    283  289  MEDLINE:95370270             7\n",
       "798173   test   ##ion  MEDLINE:95370270/7    798173         89    289  292  MEDLINE:95370270             7\n",
       "798174   test       .  MEDLINE:95370270/7    798174         90    292  293  MEDLINE:95370270             7\n",
       "798175   test   [SEP]  MEDLINE:95370270/7    798175         91    293  293  MEDLINE:95370270             7\n",
       "\n",
       "[798176 rows x 9 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ner_label</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>mention_id</th>\n",
       "      <th>text</th>\n",
       "      <th>mention_idx</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DNA</td>\n",
       "      <td>MEDLINE:95369245/0</td>\n",
       "      <td>MEDLINE:95369245</td>\n",
       "      <td>0</td>\n",
       "      <td>IL-2 gene</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>protein</td>\n",
       "      <td>MEDLINE:95369245/0</td>\n",
       "      <td>MEDLINE:95369245</td>\n",
       "      <td>1</td>\n",
       "      <td>NF-kappa B</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>protein</td>\n",
       "      <td>MEDLINE:95369245/0</td>\n",
       "      <td>MEDLINE:95369245</td>\n",
       "      <td>2</td>\n",
       "      <td>CD28</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>protein</td>\n",
       "      <td>MEDLINE:95369245/0</td>\n",
       "      <td>MEDLINE:95369245</td>\n",
       "      <td>3</td>\n",
       "      <td>5-lipoxygenase</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>protein</td>\n",
       "      <td>MEDLINE:95369245/1</td>\n",
       "      <td>MEDLINE:95369245</td>\n",
       "      <td>4</td>\n",
       "      <td>CD28</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57091</th>\n",
       "      <td>DNA</td>\n",
       "      <td>MEDLINE:95370270/7</td>\n",
       "      <td>MEDLINE:95370270</td>\n",
       "      <td>57091</td>\n",
       "      <td>AP-1 sites</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57092</th>\n",
       "      <td>protein</td>\n",
       "      <td>MEDLINE:95370270/7</td>\n",
       "      <td>MEDLINE:95370270</td>\n",
       "      <td>57092</td>\n",
       "      <td>interleukin-3</td>\n",
       "      <td>7</td>\n",
       "      <td>44</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57093</th>\n",
       "      <td>protein</td>\n",
       "      <td>MEDLINE:95370270/7</td>\n",
       "      <td>MEDLINE:95370270</td>\n",
       "      <td>57093</td>\n",
       "      <td>granulocyte macrophage colony-stimulating factor</td>\n",
       "      <td>8</td>\n",
       "      <td>51</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57094</th>\n",
       "      <td>cell_type</td>\n",
       "      <td>MEDLINE:95370270/7</td>\n",
       "      <td>MEDLINE:95370270</td>\n",
       "      <td>57094</td>\n",
       "      <td>T cell</td>\n",
       "      <td>9</td>\n",
       "      <td>70</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57095</th>\n",
       "      <td>protein</td>\n",
       "      <td>MEDLINE:95370270/7</td>\n",
       "      <td>MEDLINE:95370270</td>\n",
       "      <td>57095</td>\n",
       "      <td>interleukin-2</td>\n",
       "      <td>10</td>\n",
       "      <td>82</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57096 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ner_label         sentence_id            doc_id  mention_id                                              text  mention_idx  begin  end  depth\n",
       "0            DNA  MEDLINE:95369245/0  MEDLINE:95369245           0                                         IL-2 gene            0      1    5      0\n",
       "1        protein  MEDLINE:95369245/0  MEDLINE:95369245           1                                        NF-kappa B            1      7   13      0\n",
       "2        protein  MEDLINE:95369245/0  MEDLINE:95369245           2                                              CD28            2     15   18      0\n",
       "3        protein  MEDLINE:95369245/0  MEDLINE:95369245           3                                    5-lipoxygenase            3     23   30      0\n",
       "4        protein  MEDLINE:95369245/1  MEDLINE:95369245           4                                              CD28            0      4    7      0\n",
       "...          ...                 ...               ...         ...                                               ...          ...    ...  ...    ...\n",
       "57091        DNA  MEDLINE:95370270/7  MEDLINE:95370270       57091                                        AP-1 sites            6     34   39      1\n",
       "57092    protein  MEDLINE:95370270/7  MEDLINE:95370270       57092                                     interleukin-3            7     44   50      0\n",
       "57093    protein  MEDLINE:95370270/7  MEDLINE:95370270       57093  granulocyte macrophage colony-stimulating factor            8     51   65      0\n",
       "57094  cell_type  MEDLINE:95370270/7  MEDLINE:95370270       57094                                            T cell            9     70   72      0\n",
       "57095    protein  MEDLINE:95370270/7  MEDLINE:95370270       57095                                     interleukin-2           10     82   88      0\n",
       "\n",
       "[57096 rows x 9 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep['mentions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Dataset(\n",
      "  (docs):        2000 * ('doc_id', 'text', 'split')\n",
      "  (mentions):   57096 * ('doc_id', 'mention_id', 'label', 'text')\n",
      "  (fragments):  57096 * ('doc_id', 'mention_id', 'fragment_id', 'begin', 'end')\n",
      "  (attributes):     0 * ('doc_id', 'mention_id', 'attribute_id', 'label', 'value')\n",
      ")\n",
      "Transform texts... done\n",
      "Splitting into sentences... done\n",
      "Tokenizing...                sentence_id  token_id  token_idx   token  begin  end            doc_id  sentence_idx  split\n",
      "0       MEDLINE:95369245/0         0          0   [CLS]      0    0  MEDLINE:95369245             0  train\n",
      "1       MEDLINE:95369245/0         1          1      il      0    2  MEDLINE:95369245             0  train\n",
      "2       MEDLINE:95369245/0         2          2       -      2    3  MEDLINE:95369245             0  train\n",
      "3       MEDLINE:95369245/0         3          3       2      3    4  MEDLINE:95369245             0  train\n",
      "4       MEDLINE:95369245/0         4          4    gene      5    9  MEDLINE:95369245             0  train\n",
      "...                    ...       ...        ...     ...    ...  ...               ...           ...    ...\n",
      "798171  MEDLINE:95370270/7    798171         87       2    281  282  MEDLINE:95370270             7   test\n",
      "798172  MEDLINE:95370270/7    798172         88  secret    283  289  MEDLINE:95370270             7   test\n",
      "798173  MEDLINE:95370270/7    798173         89   ##ion    289  292  MEDLINE:95370270             7   test\n",
      "798174  MEDLINE:95370270/7    798174         90       .    292  293  MEDLINE:95370270             7   test\n",
      "798175  MEDLINE:95370270/7    798175         91   [SEP]    293  293  MEDLINE:95370270             7   test\n",
      "\n",
      "[798176 rows x 9 columns]\n",
      "done\n",
      "Processing nestings (overlapping areas)... "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c892d89f2572>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mbert_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbert_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mner_labels\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'DNA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'protein'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cell_type'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cell_line'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RNA'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0munknown_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"drop\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-10-f8058dd2fde2>\u001b[0m in \u001b[0;36mpreprocess_train\u001b[0;34m(dataset, max_sentence_length, bert_name, ner_labels, unknown_labels, vocabularies)\u001b[0m\n\u001b[1;32m     94\u001b[0m     )\n\u001b[1;32m     95\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_scipy_sparse_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_to_csr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mcolored_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoloring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgreedy_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0munique_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'depth'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolored_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolored_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mmentions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmentions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bert_name = \"bert-base-cased\"\n",
    "dataset = load_genia_ner()\n",
    "docs = dataset['docs']\n",
    "\n",
    "keep_n_first = None\n",
    "\n",
    "if keep_n_first:\n",
    "    docs = docs[:keep_n_first]\n",
    "    first_ids = docs['doc_id']\n",
    "    \n",
    "    first_mentions = dataset[\"mentions\"].loc[dataset[\"mentions\"]['doc_id'].isin(first_ids)]\n",
    "    first_fragments = dataset[\"fragments\"].loc[dataset[\"fragments\"]['doc_id'].isin(first_ids)]\n",
    "    first_attributes = dataset[\"attributes\"].loc[dataset[\"attributes\"]['doc_id'].isin(first_ids)]\n",
    "    \n",
    "    dataset[\"mentions\"] = first_mentions\n",
    "    dataset[\"fragments\"] = first_fragments\n",
    "    dataset[\"attributes\"] = first_attributes\n",
    "\n",
    "docs[\"split\"] = [\"train\"] * (len(docs) - int(len(docs) * 0.1)) + [\"test\"] * int(len(docs) * 0.1)\n",
    "dataset['docs'] = docs\n",
    "\n",
    "batcher, prep, deltas, vocs = preprocess_train(\n",
    "    dataset=dataset,\n",
    "    max_sentence_length=140,\n",
    "    bert_name=bert_name,\n",
    "    ner_labels= ['DNA', 'protein', 'cell_type', 'cell_line', 'RNA'],\n",
    "    unknown_labels=\"drop\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "bert, log = BertModel.from_pretrained(bert_name, output_loading_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity (L2 dist) between train and val frequencies: 0.004900437003164877\n",
      "Frequencies\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>DNA</th>\n",
       "      <th>RNA</th>\n",
       "      <th>cell_line</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>protein</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>0.176332</td>\n",
       "      <td>0.016980</td>\n",
       "      <td>0.068440</td>\n",
       "      <td>0.128091</td>\n",
       "      <td>0.610157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>val</td>\n",
       "      <td>0.222606</td>\n",
       "      <td>0.021301</td>\n",
       "      <td>0.081558</td>\n",
       "      <td>0.112646</td>\n",
       "      <td>0.561888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index       DNA       RNA  cell_line  cell_type   protein\n",
       "0  train  0.176332  0.016980   0.068440   0.128091  0.610157\n",
       "1    val  0.222606  0.021301   0.081558   0.112646  0.561888"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#all_test_doc_ids = []\n",
    "#sims = {}\n",
    "#for i in range(200):\n",
    "seed_all(1234567+137)\n",
    "\n",
    "train_batcher = batcher['doc'][batcher['doc']['split']==0]['sentence']\n",
    "test_batcher = batcher['doc'][batcher['doc']['split']==2]['sentence']\n",
    "\n",
    "splits = np.zeros(len(train_batcher['doc']), dtype=int)\n",
    "\n",
    "val_perc = 0.1\n",
    "splits[np.random.choice(np.arange(len(splits)), size=int(val_perc * len(splits)))] = 1\n",
    "\n",
    "val_batcher = test_batcher#batcher['sentence'][splits == 1]\n",
    "\n",
    "# train_val_split = np.random.permutation(len(train_batcher))\n",
    "# test_batcher = train_batcher[train_val_split[:int(0.1*len(train_val_split))]]['sentence']\n",
    "# train_batcher = train_batcher[train_val_split[int(0.1*len(train_val_split)):]]['sentence']\n",
    "sim = ((np.bincount(val_batcher['mention', 'ner_label'], minlength=len(vocs[\"ner_label\"]))/len(val_batcher['mention']) -\n",
    "np.bincount(train_batcher['mention', 'ner_label'], minlength=len(vocs[\"ner_label\"]))/len(train_batcher['mention']))**2).sum()\n",
    "print(\"Similarity (L2 dist) between train and val frequencies:\", sim)\n",
    "print(\"Frequencies\")\n",
    "#all_test_doc_ids.append((test_doc_ids, sim))\n",
    "display(pd.DataFrame([\n",
    "    {\"index\": \"train\", **dict(zip(vocs[\"ner_label\"], np.bincount(train_batcher['mention', 'ner_label'], minlength=len(vocs[\"ner_label\"]))/len(train_batcher['mention'])))},\n",
    "    {\"index\": \"val\", **dict(zip(vocs[\"ner_label\"], np.bincount(val_batcher['mention', 'ner_label'], minlength=len(vocs[\"ner_label\"]))/len(val_batcher['mention'])))},\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy as dc\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "def tensor_in_list(a, d):\n",
    "    return any([(a == d_).all() for d_ in d if a.shape == d_.shape])\n",
    "\n",
    "class NERNet(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 ner_labels_subset,\n",
    "                 hidden_dim, \n",
    "                 last_layer_dim,\n",
    "                 rnn_layers, \n",
    "                 dropout, \n",
    "                 embeddings=None,\n",
    "                 n_tokens=None, \n",
    "                 token_dim=None, \n",
    "                 tag_scheme=\"bio\",\n",
    "                 max_tag_depth=3,\n",
    "                 do_batch_norm=True,\n",
    "                 use_lstm=True,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        if embeddings is not None:\n",
    "            self.embeddings = embeddings\n",
    "            try:\n",
    "                n_tokens, token_dim = self.embeddings.weight.shape\n",
    "            except:\n",
    "                n_tokens, token_dim = self.embeddings.word_embeddings.weight.shape\n",
    "        else:\n",
    "            self.embeddings = torch.nn.Embedding(n_tokens, token_dim) if n_tokens > 0 else None\n",
    "        assert token_dim is not None, \"Provide token_dim or embeddings\"\n",
    "        \n",
    "        self.ner_labels_subset = ner_labels_subset\n",
    "        \n",
    "        self.lstm_dense = torch.nn.Linear(token_dim, hidden_dim*2)\n",
    "        self.lstm_dense1 = torch.nn.Linear(hidden_dim*2, hidden_dim*2)\n",
    "        if use_lstm:\n",
    "            self.lstm1 = torch.nn.LSTM(hidden_dim*2, \n",
    "                                  hidden_dim, dropout=dropout, batch_first=True, num_layers=rnn_layers, bidirectional=True)\n",
    "            \n",
    "        if tag_scheme == \"bio\":\n",
    "            self.crf_list1 = torch.nn.ModuleList([BIODecoder(1, with_start_end_transitions=False) for _ in ner_labels_subset])\n",
    "        elif tag_scheme == \"bioul\":\n",
    "            self.crf_list1 = torch.nn.ModuleList([BIOULDecoder(1, with_start_end_transitions=False) for _ in ner_labels_subset])\n",
    "        else:\n",
    "            raise Exception(f'Allowed tag schemes are \"bio\" or \"bioul\": passed was {repr(tag_scheme)}')\n",
    "\n",
    "        self.dense1 = torch.nn.Linear(hidden_dim*2, sum(crf.num_tags for crf in self.crf_list1))\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.max_tag_depth = max_tag_depth\n",
    "        \n",
    "        self.do_batch_norm = do_batch_norm\n",
    "        if do_batch_norm:\n",
    "            self.bn_embeddings = torch.nn.BatchNorm1d(token_dim)\n",
    "            self.bn_lstm = torch.nn.BatchNorm1d(hidden_dim*2)\n",
    "            \n",
    "        self.use_lstm = use_lstm\n",
    "    \n",
    "    def forward(self, \n",
    "                tokens, \n",
    "                mask, \n",
    "                tags=None, \n",
    "                return_loss=False, \n",
    "                return_mentions=False, \n",
    "                reduction=\"mean\",\n",
    "               ):\n",
    "        \n",
    "        print(\"TOKEN SHAPE\",tokens.shape)\n",
    "        # Embed the tokens\n",
    "        state = torch.cat([\n",
    "            *((self.embeddings(tokens).masked_fill(~mask.unsqueeze(-1), 0),) if self.embeddings is not None else ()),\n",
    "        ], dim=-1)\n",
    "        \n",
    "        if not self.use_lstm:\n",
    "            state = torch.relu(self.lstm_dense(self.dropout(state)))\n",
    "            state = torch.relu(self.lstm_dense1(self.dropout(state)))\n",
    "            state = torch.relu(self.dense1(self.dropout(state))).reshape((*state.shape[:-1], -1, len(self.crf_list1))).permute(3, 0, 1, 2)\n",
    "\n",
    "            mentions = {\"begin\": [], \"end\": [], \"ner_label\": [], \"doc_id\": []}\n",
    "            \n",
    "            for i, (ner_label_idx, crf) in enumerate(zip(self.ner_labels_subset, self.crf_list1)):\n",
    "                argmax_tags = crf.decode(state[i], mask)\n",
    "                extracted = crf.tags_to_spans(argmax_tags, mask)\n",
    "                mentions[\"begin\"].append(extracted[\"span_begin\"])\n",
    "                mentions[\"end\"].append(extracted[\"span_end\"])\n",
    "                mentions[\"ner_label\"].append(torch.full_like(extracted[\"span_label\"], ner_label_idx))\n",
    "                mentions[\"doc_id\"].append(extracted[\"span_doc_id\"])\n",
    "                \n",
    "            mentions[\"begin\"] = torch.cat(mentions[\"begin\"])\n",
    "            mentions[\"end\"] = torch.cat(mentions[\"end\"])\n",
    "            mentions[\"ner_label\"] = torch.cat(mentions[\"ner_label\"])\n",
    "            mentions[\"doc_id\"] = torch.cat(mentions[\"doc_id\"])\n",
    "\n",
    "            if return_loss:\n",
    "                loss = -sum([crf(state[i], mask, tags[..., ner_label_idx], reduction=\"none\") \n",
    "                                for i, (ner_label_idx, crf) in enumerate(zip(self.ner_labels_subset, self.crf_list1))]).mean()# / len(self.crf_list)\n",
    "            return {\n",
    "                # Run the linear CRF forward algorithm on the tokens to compute the loglikelihood of the targets\n",
    "                \"loss\": sum([loss]) if return_loss else sum([]), \n",
    "                # Run the linear CRF Viterbi algorithm to compute the most likely sequence\n",
    "                **mentions\n",
    "            }\n",
    "                \n",
    "            \n",
    "        # Run the lstm (first sort the sentences, pack them according to the mask,\n",
    "        # unpack them and finally reorder them)\n",
    "        sorter = torch.argsort(-mask.sum(1))\n",
    "        invsorter = torch.argsort(sorter)\n",
    "        \n",
    "        \n",
    "        print(\"STATE SHAPE BEFORE VIEW\", state.shape)\n",
    "        \n",
    "        # batch_norm\n",
    "        if self.do_batch_norm:\n",
    "            normed_state = self.bn_embeddings(state.permute(0, 2, 1))\n",
    "            # Compute the tags scores\n",
    "            state_lstm = self.lstm_dense(normed_state.permute(0, 2, 1))\n",
    "        else:\n",
    "            state_lstm = self.lstm_dense(state)\n",
    "            \n",
    "        # FIRST LAYER OUTPUT COMPUTATION\n",
    "        all_mentions = {\"begin\": [], \"end\": [], \"ner_label\": [], \"doc_id\": []}\n",
    "        all_losses = []\n",
    "        \n",
    "        print(state_lstm.shape)\n",
    "        print(pack_padded_sequence(state_lstm[sorter], mask[sorter].sum(1), batch_first=True))\n",
    "        \n",
    "        for tag_layer in range(self.max_tag_depth):\n",
    "            state_lstm = pad_packed_sequence(\n",
    "                self.lstm1(pack_padded_sequence(state_lstm[sorter] if tag_layer==0 else next_layer_input, mask[sorter].sum(1), batch_first=True))[0], \n",
    "                batch_first=True)[0]\n",
    "            \n",
    "            print(\"STATE LSTM SHAPE BEFORE VIEW\", state_lstm.shape)\n",
    "            \n",
    "            state_lstm = state_lstm.view(*state_lstm.shape[:2], -1)[invsorter]\n",
    "            \n",
    "            print(\"STATE LSTM SHAPE AFTER VIEW\", state_lstm.shape)\n",
    "            \n",
    "            if self.do_batch_norm:\n",
    "                state_lstm = self.bn_lstm(state_lstm.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "            \n",
    "            # Compute the tags scores\n",
    "            state = self.dense1(self.dropout(state_lstm)).reshape((*state_lstm.shape[:-1], -1, len(self.crf_list1))).permute(3, 0, 1, 2)\n",
    "            \n",
    "            mentions = {\"begin\": [], \"end\": [], \"ner_label\": [], \"doc_id\": []}\n",
    "            \n",
    "            for i, (ner_label_idx, crf) in enumerate(zip(self.ner_labels_subset, self.crf_list1)):\n",
    "                argmax_tags = crf.decode(state[i], mask)\n",
    "                extracted = crf.tags_to_spans(argmax_tags, mask)\n",
    "                mentions[\"begin\"].append(extracted[\"span_begin\"])\n",
    "                mentions[\"end\"].append(extracted[\"span_end\"])\n",
    "                mentions[\"ner_label\"].append(torch.full_like(extracted[\"span_label\"], ner_label_idx))\n",
    "                mentions[\"doc_id\"].append(extracted[\"span_doc_id\"])\n",
    "                \n",
    "            for k in mentions:\n",
    "                all_mentions[k].extend(mentions[k])\n",
    "                \n",
    "            mentions[\"begin\"] = torch.cat(mentions[\"begin\"])\n",
    "            mentions[\"end\"] = torch.cat(mentions[\"end\"])\n",
    "            mentions[\"ner_label\"] = torch.cat(mentions[\"ner_label\"])\n",
    "            mentions[\"doc_id\"] = torch.cat(mentions[\"doc_id\"])\n",
    "\n",
    "            if return_loss:\n",
    "                loss1 = -sum([crf(state[i], mask, tags[..., ner_label_idx], reduction=\"none\") \n",
    "                                for i, (ner_label_idx, crf) in enumerate(zip(self.ner_labels_subset, self.crf_list1))]).mean()# / len(self.crf_list)\n",
    "            \n",
    "                all_losses.append(loss1)\n",
    "            \n",
    "            if len(mentions[\"begin\"]) == 0:\n",
    "                for k in all_mentions:\n",
    "                    if len(all_mentions[k]):\n",
    "                        all_mentions[k] = torch.cat(all_mentions[k])\n",
    "            \n",
    "                return {\n",
    "                    # Run the linear CRF forward algorithm on the tokens to compute the loglikelihood of the targets\n",
    "                    \"loss\": sum(all_losses), \n",
    "                    # Run the linear CRF Viterbi algorithm to compute the most likely sequence\n",
    "                    **all_mentions\n",
    "                }#, {\"loss\":loss1, **all_mentions}\n",
    "            \n",
    "            # token_to_mention_mask => shape #n_sentences, sentence_size, n_mentions\n",
    "#             token_to_mention_mask = torch.ones((*state_lstm.shape[:2], len(mentions['begin'])), device=state_lstm.device, dtype=torch.bool)\n",
    "#             mentions_length = [e - b for b, e in zip(mentions[\"begin\"], mentions[\"end\"])]\n",
    "            \n",
    "#             mentions_state = torch.einsum('nld,nlm->dm', state_lstm, token_to_mention_mask.float())\n",
    "            \n",
    "#             mentions_state = mentions_state / torch.Tensor(mentions_length).to(state_lstm.device)\n",
    "            \n",
    "#             next_layer_input = state_lstm.clone()\n",
    "#             next_layer_input[token_to_mention_mask.any(2)] = torch.einsum('dm,nlm->nld', mentions_state, token_to_mention_mask.float())[token_to_mention_mask.any(2)]\n",
    "            \n",
    "            next_layer_input = state_lstm.clone()\n",
    "        \n",
    "            for b, e in zip(mentions[\"begin\"], mentions[\"end\"]):\n",
    "                # average representation of each detected mention\n",
    "                next_layer_input[:,b:e].data = next_layer_input[:,b:e].clone().mean() #(1/1+e-b) * state_lstm[:,b:e].sum()\n",
    "            \n",
    "        for k in all_mentions:\n",
    "            if len(all_mentions[k]):\n",
    "                all_mentions[k] = torch.cat(all_mentions[k])\n",
    "\n",
    "        return {\n",
    "            # Run the linear CRF forward algorithm on the tokens to compute the loglikelihood of the targets\n",
    "            \"loss\": sum(all_losses), \n",
    "            # Run the linear CRF Viterbi algorithm to compute the most likely sequence\n",
    "            **all_mentions\n",
    "        }#, {\"loss\": first_loss, **first_mentions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install /home/yoann/these/DEFT/nlstruct/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CUDA devices 8\n",
      "Current device cuda\n",
      "Using cache /home/tannier/data/cache/genia/7dfa167ed717d361\n",
      "Loading /home/tannier/data/cache/genia/7dfa167ed717d361/history.yaml... Done\n",
      "\n",
      "\n",
      "epoch |    dur(s) |       lr | n_matched | train_f1 | train_ner_loss | \u001b[31mval_f1\u001b[0m\n",
      "    1 |  119.7527 | 7.00e-03 |    0.0000 |   \u001b[32m0.1116\u001b[0m |       \u001b[32m174.4086\u001b[0m | \u001b[32m0.3621\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tannier/anaconda3/envs/yt_nlp/lib/python3.7/site-packages/nlstruct/train/helpers.py:138: UserWarning: Entry 'schedules' in the state seems to be mutable but has no load_state_dict/state_dict methods. This could lead to unpredictable behaviors.\n",
      "  warn(f\"Entry '{key}' in the state seems to be mutable but has no load_state_dict/state_dict methods. This could lead to unpredictable behaviors.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2 |   71.5648 | 7.00e-03 |    0.0000 |   \u001b[32m0.6540\u001b[0m |        \u001b[32m45.5370\u001b[0m | \u001b[32m0.5329\u001b[0m\n",
      "    3 |   72.6780 | 7.00e-03 |    0.0000 |   \u001b[32m0.6773\u001b[0m |        \u001b[32m44.7306\u001b[0m | \u001b[32m0.5343\u001b[0m\n",
      "    4 |   72.4068 | 7.00e-03 |    0.0000 |   \u001b[32m0.6924\u001b[0m |        \u001b[32m41.9649\u001b[0m | \u001b[32m0.5945\u001b[0m\n",
      "    5 |   71.5367 | 7.00e-03 |    0.0000 |   \u001b[32m0.6983\u001b[0m |        \u001b[32m41.0320\u001b[0m | \u001b[32m0.6203\u001b[0m\n",
      "    6 |   67.4816 | 7.00e-03 |    0.0000 |   \u001b[31m0.6938\u001b[0m |        \u001b[31m41.0912\u001b[0m | \u001b[31m0.5512\u001b[0m\n",
      "    7 |   69.0711 | 7.00e-03 |    0.0000 |   \u001b[32m0.7038\u001b[0m |        \u001b[31m41.0858\u001b[0m | \u001b[31m0.6140\u001b[0m\n",
      "    8 |   71.6682 | 7.00e-03 |    0.0000 |   \u001b[32m0.7097\u001b[0m |        \u001b[32m39.6520\u001b[0m | \u001b[32m0.6367\u001b[0m\n",
      "    9 |   71.5270 | 7.00e-03 |    0.0000 |   \u001b[31m0.7097\u001b[0m |        \u001b[31m40.3215\u001b[0m | \u001b[31m0.5495\u001b[0m\n",
      "   10 |   73.0896 | 7.00e-03 |    0.0000 |   \u001b[32m0.7127\u001b[0m |        \u001b[31m39.8914\u001b[0m | \u001b[31m0.5837\u001b[0m\n",
      "   11 |   71.7698 | 7.00e-03 |    0.0000 |   \u001b[32m0.7143\u001b[0m |        \u001b[32m38.5646\u001b[0m | \u001b[31m0.6361\u001b[0m\n",
      "   12 |   70.5173 | 7.00e-03 |    0.0000 |   \u001b[32m0.7150\u001b[0m |        \u001b[31m38.7304\u001b[0m | \u001b[31m0.6208\u001b[0m\n",
      "   13 |   72.2161 | 7.00e-03 |    0.0000 |   \u001b[32m0.7191\u001b[0m |        \u001b[32m38.3908\u001b[0m | \u001b[31m0.6277\u001b[0m\n",
      "   14 |   72.2939 | 7.00e-03 |    0.0000 |   \u001b[31m0.7157\u001b[0m |            \u001b[31mnan\u001b[0m | \u001b[32m0.6452\u001b[0m\n",
      "   15 |   70.2366 | 1.75e-03 |    0.0000 |   \u001b[32m0.7195\u001b[0m |        \u001b[31m38.5812\u001b[0m | \u001b[31m0.6427\u001b[0m\n",
      "   16 |   74.1625 | 1.75e-03 |    0.0000 |   \u001b[32m0.7312\u001b[0m |        \u001b[32m36.6094\u001b[0m | \u001b[32m0.6586\u001b[0m\n",
      "   17 |   72.7413 | 1.75e-03 |    0.0000 |   \u001b[32m0.7412\u001b[0m |        \u001b[32m36.4482\u001b[0m | \u001b[32m0.6652\u001b[0m\n",
      "   18 |   72.7817 | 1.75e-03 |    0.0000 |   \u001b[32m0.7452\u001b[0m |        \u001b[32m35.6200\u001b[0m | \u001b[31m0.6636\u001b[0m\n",
      "   19 |   71.7094 | 1.75e-03 |    0.0000 |   \u001b[32m0.7474\u001b[0m |        \u001b[31m35.7669\u001b[0m | \u001b[32m0.6737\u001b[0m\n",
      "   20 |   70.7574 | 1.75e-03 |    0.0000 |   \u001b[32m0.7503\u001b[0m |        \u001b[32m35.2833\u001b[0m | \u001b[31m0.6728\u001b[0m\n",
      "   21 |   73.1659 | 1.75e-03 |    0.0000 |   \u001b[32m0.7530\u001b[0m |        \u001b[32m34.7648\u001b[0m | \u001b[32m0.6884\u001b[0m\n",
      "   22 |   69.8712 | 1.75e-03 |    0.0000 |   \u001b[32m0.7539\u001b[0m |        \u001b[31m35.1620\u001b[0m | \u001b[31m0.6777\u001b[0m\n",
      "   23 |   72.4528 | 1.75e-03 |    0.0000 |   \u001b[32m0.7577\u001b[0m |        \u001b[32m34.6502\u001b[0m | \u001b[32m0.6908\u001b[0m\n",
      "   24 |   71.4816 | 1.75e-03 |    0.0000 |   \u001b[32m0.7619\u001b[0m |        \u001b[32m34.6308\u001b[0m | \u001b[31m0.6797\u001b[0m\n",
      "   25 |   73.0417 | 1.75e-03 |    0.0000 |   \u001b[32m0.7638\u001b[0m |        \u001b[32m34.1668\u001b[0m | \u001b[31m0.6780\u001b[0m\n",
      "   26 |   69.5739 | 1.75e-03 |    0.0000 |   \u001b[32m0.7640\u001b[0m |        \u001b[32m33.7618\u001b[0m | \u001b[31m0.6899\u001b[0m\n",
      "   27 |   73.2269 | 1.75e-03 |    0.0000 |   \u001b[32m0.7675\u001b[0m |        \u001b[31m33.9944\u001b[0m | \u001b[31m0.6799\u001b[0m\n",
      "   28 |   72.4134 | 1.75e-03 |    0.0000 |   \u001b[31m0.7672\u001b[0m |        \u001b[31m33.9679\u001b[0m | \u001b[31m0.6852\u001b[0m\n",
      "   29 |   73.2792 | 1.75e-03 |    0.0000 |   \u001b[32m0.7706\u001b[0m |        \u001b[32m33.5481\u001b[0m | \u001b[31m0.6793\u001b[0m\n",
      "   30 |   73.2591 | 4.38e-04 |    0.0000 |   \u001b[31m0.7693\u001b[0m |        \u001b[32m33.3034\u001b[0m | \u001b[31m0.6898\u001b[0m\n",
      "   31 |   72.1119 | 4.38e-04 |    0.0000 |   \u001b[32m0.7746\u001b[0m |        \u001b[32m33.0919\u001b[0m | \u001b[31m0.6907\u001b[0m\n",
      "   32 |   71.9811 | 4.38e-04 |    0.0000 |   \u001b[31m0.7743\u001b[0m |        \u001b[32m33.0640\u001b[0m | \u001b[31m0.6901\u001b[0m\n",
      "   33 |   73.3579 | 4.38e-04 |    0.0000 |   \u001b[32m0.7760\u001b[0m |        \u001b[31m33.3343\u001b[0m | \u001b[31m0.6870\u001b[0m\n",
      "   34 |   73.9660 | 4.38e-04 |    0.0000 |   \u001b[32m0.7764\u001b[0m |        \u001b[32m32.7642\u001b[0m | \u001b[32m0.6929\u001b[0m\n",
      "   35 |   73.1420 | 4.38e-04 |    0.0000 |   \u001b[32m0.7773\u001b[0m |        \u001b[31m32.8726\u001b[0m | \u001b[32m0.6956\u001b[0m\n",
      "   36 |   71.4766 | 4.38e-04 |    0.0000 |   \u001b[32m0.7800\u001b[0m |        \u001b[32m32.6420\u001b[0m | \u001b[31m0.6910\u001b[0m\n",
      "   37 |   72.2139 | 4.38e-04 |    0.0000 |   \u001b[31m0.7798\u001b[0m |        \u001b[31m32.8810\u001b[0m | \u001b[32m0.6963\u001b[0m\n",
      "   38 |   72.8794 | 4.38e-04 |    0.0000 |   \u001b[32m0.7806\u001b[0m |        \u001b[31m32.8704\u001b[0m | \u001b[31m0.6942\u001b[0m\n",
      "   39 |   73.0196 | 4.38e-04 |    0.0000 |   \u001b[31m0.7800\u001b[0m |        \u001b[31m32.9037\u001b[0m | \u001b[31m0.6928\u001b[0m\n",
      "   40 |   72.7876 | 1.09e-04 |    0.0000 |   \u001b[32m0.7843\u001b[0m |        \u001b[31m33.1258\u001b[0m | \u001b[32m0.6974\u001b[0m\n",
      "   41 |   72.2163 | 1.09e-04 |    0.0000 |   \u001b[31m0.7832\u001b[0m |        \u001b[31m32.6639\u001b[0m | \u001b[31m0.6928\u001b[0m\n",
      "   42 |   71.0146 | 1.09e-04 |    0.0000 |   \u001b[32m0.7851\u001b[0m |        \u001b[32m32.5588\u001b[0m | \u001b[31m0.6910\u001b[0m\n",
      "   43 |   72.7618 | 1.09e-04 |    0.0000 |   \u001b[31m0.7832\u001b[0m |        \u001b[32m32.5526\u001b[0m | \u001b[31m0.6854\u001b[0m\n",
      "   44 |   73.2511 | 1.09e-04 |    0.0000 |   \u001b[31m0.7830\u001b[0m |        \u001b[32m32.3595\u001b[0m | \u001b[31m0.6923\u001b[0m\n",
      "   45 |   71.6684 | 1.09e-04 |    0.0000 |   \u001b[31m0.7845\u001b[0m |        \u001b[31m32.4673\u001b[0m | \u001b[31m0.6928\u001b[0m\n",
      "   46 |   67.3839 | 1.09e-04 |    0.0000 |   \u001b[31m0.7840\u001b[0m |        \u001b[31m32.9060\u001b[0m | \u001b[31m0.6861\u001b[0m\n",
      "   47 |   72.5739 | 1.09e-04 |    0.0000 |   \u001b[31m0.7851\u001b[0m |        \u001b[31m32.7623\u001b[0m | \u001b[31m0.6908\u001b[0m\n",
      "   48 |   71.6648 | 1.09e-04 |    0.0000 |   \u001b[31m0.7839\u001b[0m |        \u001b[31m32.7505\u001b[0m | \u001b[31m0.6948\u001b[0m\n",
      "   49 |   71.7672 | 1.09e-04 |    0.0000 |   \u001b[31m0.7835\u001b[0m |        \u001b[31m32.6356\u001b[0m | \u001b[31m0.6933\u001b[0m\n",
      "   50 |   70.9684 | 1.09e-04 |    0.0000 |   \u001b[31m0.7844\u001b[0m |        \u001b[31m32.5975\u001b[0m | \u001b[31m0.6850\u001b[0m\n",
      "   51 |   73.6739 | 7.00e-03 |    0.0000 |   \u001b[31m0.7664\u001b[0m |        \u001b[31m67.0810\u001b[0m | \u001b[31m0.6898\u001b[0m\n",
      "   52 |   72.0226 | 7.00e-03 |    0.0000 |   \u001b[31m0.7568\u001b[0m |        \u001b[31m69.3455\u001b[0m | \u001b[31m0.6954\u001b[0m\n",
      "   53 |   72.0565 | 7.00e-03 |    0.0000 |   \u001b[31m0.7576\u001b[0m |        \u001b[31m68.2647\u001b[0m | \u001b[31m0.6847\u001b[0m\n",
      "   54 |   80.3601 | 7.00e-03 |    0.0000 |   \u001b[31m0.7556\u001b[0m |        \u001b[31m67.8489\u001b[0m | \u001b[31m0.6774\u001b[0m\n",
      "   55 |   73.0983 | 7.00e-03 |    0.0000 |   \u001b[31m0.7523\u001b[0m |        \u001b[31m68.2244\u001b[0m | \u001b[31m0.6762\u001b[0m\n",
      "   56 |   69.3261 | 7.00e-03 |    0.0000 |   \u001b[31m0.7512\u001b[0m |        \u001b[31m67.8910\u001b[0m | \u001b[31m0.6869\u001b[0m\n",
      "   57 |   71.5488 | 7.00e-03 |    0.0000 |   \u001b[31m0.7517\u001b[0m |        \u001b[31m68.2588\u001b[0m | \u001b[31m0.6654\u001b[0m\n",
      "   58 |   73.5257 | 7.00e-03 |    0.0000 |   \u001b[31m0.7518\u001b[0m |        \u001b[31m68.5725\u001b[0m | \u001b[31m0.6713\u001b[0m\n",
      "   59 |   70.9299 | 7.00e-03 |    0.0000 |   \u001b[31m0.7534\u001b[0m |        \u001b[31m68.4463\u001b[0m | \u001b[31m0.6889\u001b[0m\n",
      "   60 |   71.5388 | 7.00e-03 |    0.0000 |   \u001b[31m0.7505\u001b[0m |        \u001b[31m68.1008\u001b[0m | \u001b[31m0.6761\u001b[0m\n",
      "   61 |   72.1401 | 7.00e-03 |    0.0000 |   \u001b[31m0.7515\u001b[0m |        \u001b[31m67.7794\u001b[0m | \u001b[31m0.6916\u001b[0m\n",
      "   62 |   72.5283 | 7.00e-03 |    0.0000 |   \u001b[31m0.7485\u001b[0m |        \u001b[31m68.3164\u001b[0m | \u001b[31m0.6914\u001b[0m\n",
      "   63 |   71.3393 | 7.00e-03 |    0.0000 |   \u001b[31m0.7515\u001b[0m |        \u001b[31m68.1323\u001b[0m | \u001b[31m0.6875\u001b[0m\n",
      "   64 |   71.2704 | 7.00e-03 |    0.0000 |   \u001b[31m0.7487\u001b[0m |        \u001b[31m67.6032\u001b[0m | \u001b[31m0.6816\u001b[0m\n",
      "   65 |   71.4602 | 1.75e-03 |    0.0000 |   \u001b[31m0.7496\u001b[0m |        \u001b[31m67.9237\u001b[0m | \u001b[31m0.6817\u001b[0m\n",
      "   66 |   72.6880 | 1.75e-03 |    0.0000 |   \u001b[31m0.7578\u001b[0m |        \u001b[31m67.3906\u001b[0m | \u001b[31m0.6840\u001b[0m\n",
      "   67 |   72.6796 | 1.75e-03 |    0.0000 |   \u001b[31m0.7651\u001b[0m |        \u001b[31m66.4311\u001b[0m | \u001b[31m0.6900\u001b[0m\n",
      "   68 |   68.1661 | 1.75e-03 |    0.0000 |   \u001b[31m0.7671\u001b[0m |        \u001b[31m66.4258\u001b[0m | \u001b[31m0.6931\u001b[0m\n",
      "   69 |   72.8070 | 1.75e-03 |    0.0000 |   \u001b[31m0.7700\u001b[0m |        \u001b[31m66.5672\u001b[0m | \u001b[31m0.6934\u001b[0m\n",
      "   70 |   71.1079 | 1.75e-03 |    0.0000 |   \u001b[31m0.7736\u001b[0m |        \u001b[31m65.8283\u001b[0m | \u001b[31m0.6906\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/66 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   71 |   69.7193 | 1.75e-03 |    0.0000 |   \u001b[31m0.7740\u001b[0m |        \u001b[31m66.0823\u001b[0m | \u001b[31m0.6909\u001b[0m\n",
      "TOKEN SHAPE torch.Size([257, 18])\n",
      "STATE SHAPE BEFORE VIEW torch.Size([257, 18, 768])\n",
      "torch.Size([257, 18, 200])\n",
      "PackedSequence(data=tensor([[ 0.5043,  0.4335, -0.0854,  ...,  0.0195,  0.3043, -0.3007],\n",
      "        [ 0.3785,  0.5840,  0.1604,  ..., -0.2488, -0.0015, -0.3719],\n",
      "        [ 0.3987,  0.5234, -0.0079,  ..., -0.1260,  0.2055, -0.6104],\n",
      "        ...,\n",
      "        [-0.1935,  0.2165,  0.1457,  ...,  0.0651, -0.2512, -0.1704],\n",
      "        [ 0.1180,  0.0531, -0.0799,  ...,  0.2969, -0.1157,  0.1206],\n",
      "        [ 0.1415, -0.1061,  0.1540,  ..., -0.0825,  0.0643,  0.1110]],\n",
      "       device='cuda:0', grad_fn=<PackPaddedSequenceBackward>), batch_sizes=tensor([257, 257, 257, 257, 257, 257, 257, 257, 257, 257, 257, 257, 255, 250,\n",
      "        235, 188, 110,  35]), sorted_indices=None, unsorted_indices=None)\n",
      "STATE LSTM SHAPE BEFORE VIEW torch.Size([257, 18, 200])\n",
      "STATE LSTM SHAPE AFTER VIEW torch.Size([257, 18, 200])\n",
      "STATE LSTM SHAPE BEFORE VIEW torch.Size([257, 18, 200])\n",
      "STATE LSTM SHAPE AFTER VIEW torch.Size([257, 18, 200])\n",
      "STATE LSTM SHAPE BEFORE VIEW torch.Size([257, 18, 200])\n",
      "STATE LSTM SHAPE AFTER VIEW torch.Size([257, 18, 200])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/66 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-0db6448625fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m           \u001b[0mtarget_tags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0mreturn_mentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                 \u001b[0mreturn_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             )\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-d35892eb1fc9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens, mask, tags, return_loss, return_mentions, reduction)\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmentions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"begin\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmentions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"end\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;31m# average representation of each detected mention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m                 \u001b[0mnext_layer_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_layer_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(1/1+e-b) * state_lstm[:,b:e].sum()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_mentions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "from tqdm import tqdm\n",
    "\n",
    "from custom_bert import CustomBertModel\n",
    "from transformers import AdamW, BertModel\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "from logic_crf import CRF, ConstraintFactor, HintFactor, Indexer\n",
    "\n",
    "from nlstruct.environment import get_cache\n",
    "from nlstruct.utils import evaluating, torch_global as tg, freeze\n",
    "from nlstruct.scoring import compute_metrics, merge_pred_and_gold\n",
    "from nlstruct.train import make_optimizer_and_schedules, iter_optimization, seed_all\n",
    "from nlstruct.train.schedule import ScaleOnPlateauSchedule, LinearSchedule, ConstantSchedule\n",
    "\n",
    "from nlstruct.utils import torch_clone\n",
    "    \n",
    "device = torch.device('cuda')\n",
    "tg.set_device(device)\n",
    "all_preds = []\n",
    "histories = []\n",
    "\n",
    "# To release gpu memory before allocating new parameters for a new model\n",
    "# A better idea would be to run xp in a function, so that all variables are released when exiting the fn\n",
    "# but this way we can debug after this cell if something goes wrong\n",
    "if \"all_nets\" in globals(): del all_nets\n",
    "if \"optim\" in globals(): del optim, \n",
    "if \"schedules\" in globals(): del schedules\n",
    "if \"final_schedule\" in globals(): del final_schedule\n",
    "if \"state\" in globals(): del state\n",
    "    \n",
    "# Hyperparameter search\n",
    "layer, hidden_dim, scheme, seed, lr, bert_lr, n_schedules, dropout = (13, 100, \"bioul\", 12,  7e-3, 6e-5, 4, 0.1)\n",
    "#seed = 123456\n",
    "seed_all(seed) # /!\\ Super important to enable reproducibility\n",
    "\n",
    "max_grad_norm = 5.\n",
    "#lr = 1e-3\n",
    "#bert_lr = 6e-5\n",
    "tags_lr = bert_lr\n",
    "bert_weight_decay = 0.0000\n",
    "batch_size = 257\n",
    "random_perm=True\n",
    "observed_zone_sizes=None\n",
    "n_per_zone = \"uniform\"\n",
    "n_freeze = layer + 2#4\n",
    "#hidden_dim = 256\n",
    "bert_dropout = 0.2\n",
    "top_dropout = dropout\n",
    "\n",
    "dim = 100\n",
    "\n",
    "ner_net = NERNet(\n",
    "        n_tokens=len(vocs[\"token\"]),\n",
    "        token_dim=100,#768,\n",
    "#             n_labels=len(vocs[\"ner_label\"]),\n",
    "        embeddings=torch_clone(bert.embeddings),\n",
    "        dropout=top_dropout,\n",
    "        hidden_dim=hidden_dim,\n",
    "        tag_scheme=scheme,\n",
    "        ner_labels_subset=list(range(len(vocs[\"ner_label\"]))), \n",
    "        last_layer_dim=hidden_dim, \n",
    "        rnn_layers=2, \n",
    "#             metric='linear' # cosine might be better but looks less stable, oddly,\n",
    ")\n",
    "all_nets = torch.nn.ModuleDict({\n",
    "    \"ner_net\": ner_net,\n",
    "}).to(device=tg.device)\n",
    "del ner_net\n",
    "\n",
    "for module in all_nets[\"ner_net\"].embeddings.modules():\n",
    "    if isinstance(module, torch.nn.Dropout):\n",
    "        module.p = bert_dropout\n",
    "all_nets.train()\n",
    "\n",
    "# Define the optimizer, maybe multiple learning rate / schedules per parameters groups\n",
    "optim, schedules = make_optimizer_and_schedules(all_nets, AdamW, {\n",
    "    \"lr\": [\n",
    "                           (lr,    bert_lr,    bert_lr,    tags_lr),\n",
    "        (ConstantSchedule, (lr,    bert_lr,    bert_lr,    tags_lr),    15),\n",
    "        (ConstantSchedule, (lr/4,  bert_lr/4,  bert_lr/4,  tags_lr/4),  15),\n",
    "        (ConstantSchedule, (lr/16, bert_lr/16, bert_lr/16, tags_lr/16), 10),\n",
    "        (ConstantSchedule, (lr/64, bert_lr/64, bert_lr/64, tags_lr/64), 10),\n",
    "    ][:n_schedules+1],\n",
    "}, [\n",
    "    \"(?!ner_net\\.embeddings\\.).*\", \n",
    "    \"ner_net\\.embeddings\\..*(bias|LayerNorm\\.weight)\",\n",
    "    \"ner_net\\.embeddings\\..*(?!bias|LayerNorm\\.weight)\",\n",
    "], num_iter_per_epoch=(len(train_batcher) + 1) / batch_size)\n",
    "final_schedule = ScaleOnPlateauSchedule('lr', optim, patience=4, factor=0.25, verbose=True, mode='max')\n",
    "\n",
    "# Freeze some bert layers \n",
    "# - n_freeze = 0 to freeze nothing\n",
    "# - n_freeze = 1 to freeze word embeddings / position embeddings / ...\n",
    "# - n_freeze = 2..13 to freeze the first, second ... 12th layer of bert\n",
    "for name, param in all_nets.named_parameters():\n",
    "    match = re.search(\"\\.(\\d+)\\.\", name)\n",
    "    if match and int(match.group(1)) < n_freeze - 1:\n",
    "        freeze([param])\n",
    "if n_freeze > 0:\n",
    "    if hasattr(all_nets['ner_net'].embeddings, 'embeddings'):\n",
    "        freeze(all_nets['ner_net'].embeddings.embeddings)\n",
    "    else:\n",
    "        freeze(all_nets['ner_net'].embeddings)\n",
    "\n",
    "with_tqdm = True\n",
    "state = {\"all_nets\": all_nets, \"optim\": optim, \"schedules\": schedules, \"final_schedule\": final_schedule}  # all we need to restart the training from a given epoch\n",
    "\n",
    "for epoch_before, state, history, record in iter_optimization(\n",
    "    main_score = \"val_f1\", # do not earlystop based on validation\n",
    "    metrics_info=metrics_info,\n",
    "    max_epoch=81,\n",
    "    patience=50,\n",
    "    state=state, \n",
    "    cache_policy=\"all\", # only store metrics, not checkpoints\n",
    "    cache=get_cache(\"genia\", {\"seed\": seed, \"train_batcher\": train_batcher, \"val_batcher\": None, \"random_perm\": random_perm, \"observed_zone_sizes\": observed_zone_sizes, \"batch_size\": batch_size, \"max_grad_norm\": max_grad_norm, **state}, loader=torch.load, dumper=torch.save),  # where to store the model (main name + hashed parameters)\n",
    "    n_save_checkpoints=2,\n",
    "#             exit_on_score=0.92,\n",
    "):\n",
    "    pred_batches = []\n",
    "    gold_batches = []\n",
    "\n",
    "    total_train_ner_loss = 0\n",
    "    total_train_acc = 0\n",
    "    total_train_ner_size = 0\n",
    "\n",
    "    n_mentions = len(train_batcher[\"mention\"])\n",
    "    n_matched_mentions = 0\n",
    "    n_target_mentions = 0\n",
    "    n_observed_mentions = 0\n",
    "\n",
    "    with tqdm(train_batcher['sentence'].dataloader(batch_size=batch_size, shuffle=True, sparse_sort_on=\"token_mask\", device=device), disable=not with_tqdm) as bar:\n",
    "        for batch_i, batch in enumerate(bar):\n",
    "            optim.zero_grad()\n",
    "\n",
    "            n_samples, sentence_size = batch[\"sentence\", \"token\"].shape\n",
    "            n_labels = len(vocs[\"ner_label\"])\n",
    "            \n",
    "            mask = batch[\"token_mask\"]\n",
    "\n",
    "            # Compute the tokens label tag of the selected non-overlapping gold mentions to infer from the model\n",
    "            target_tags = all_nets[\"ner_net\"].crf_list1[0].spans_to_tags(\n",
    "                batch[\"mention\", \"@sentence_id\"] * n_labels + batch[\"mention\", \"ner_label\"],\n",
    "                batch[\"mention\", \"begin\"],\n",
    "                batch[\"mention\", \"end\"], \n",
    "                torch.zeros_like(batch[\"mention\", \"ner_label\"]), \n",
    "                n_tokens=sentence_size,\n",
    "                n_samples=n_samples * n_labels,\n",
    "            ) # [n_samples * n_labels] * sentence_size\n",
    "\n",
    "            target_tags = target_tags.view(n_samples, n_labels, sentence_size)\n",
    "            target_tags = target_tags.transpose(1, 2)\n",
    "            \n",
    "            # WITH DEPTH\n",
    "            \n",
    "#             # Compute the tokens label tag of the selected non-overlapping gold mentions to infer from the model\n",
    "#             n_depth = batch[\"mention\", \"depth\"].max().item() + 2\n",
    "#             target_tags = all_nets[\"ner_net\"].crf_list1[0].spans_to_tags(\n",
    "#                 batch[\"mention\", \"@sentence_id\"] + batch[\"mention\", \"depth\"] * n_samples,\n",
    "#                 batch[\"mention\", \"begin\"],\n",
    "#                 batch[\"mention\", \"end\"], \n",
    "#                 batch[\"mention\", \"ner_label\"], \n",
    "#                 n_tokens=sentence_size,\n",
    "#                 n_samples=n_samples * n_depth,\n",
    "#             ) # [n_samples * n_labels] * sentence_size\n",
    "#             target_tags = target_tags.view(n_depth, n_samples, sentence_size)\n",
    "\n",
    "            res = all_nets[\"ner_net\"].forward(\n",
    "                tokens =         batch[\"token\"],\n",
    "                mask =           mask,\n",
    "                tags =           target_tags,\n",
    "                return_mentions=True,\n",
    "                return_loss=True,\n",
    "            )\n",
    "\n",
    "            loss = res['loss']\n",
    "            total_train_ner_loss += float(loss) * len(batch[\"sentence\"])\n",
    "            total_train_ner_size += len(batch[\"sentence\"])\n",
    "\n",
    "            # Perform optimization step\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(all_nets.parameters(), max_grad_norm)\n",
    "            optim.step()\n",
    "            for schedule_name, schedule in schedules.items():\n",
    "                schedule.step()\n",
    "                \n",
    "            pred_batch = Batcher({\n",
    "                \"mention\": {\n",
    "                    \"mention_id\": torch.arange(n_mentions, n_mentions+len(res['doc_id']), device=device),\n",
    "                    \"begin\": res[\"begin\"],\n",
    "                    \"end\": res[\"end\"],\n",
    "                    \"ner_label\": res[\"ner_label\"],\n",
    "                    \"@sentence_id\": res[\"doc_id\"],\n",
    "                },\n",
    "                \"sentence\": dict(batch[\"sentence\", [\"sentence_id\", \"doc_id\"]]),\n",
    "                \"doc\": dict(batch[\"doc\"])}, \n",
    "                check=False)\n",
    "            pred_batches.append(pred_batch)\n",
    "            n_mentions += len(res['doc_id'])\n",
    "            \n",
    "    \n",
    "    val_batches = []\n",
    "    \n",
    "    with tqdm(val_batcher['sentence'].dataloader(batch_size=batch_size, shuffle=True, sparse_sort_on=\"token_mask\", device=device), disable=not with_tqdm) as bar:\n",
    "        for batch_i, batch in enumerate(bar):\n",
    "            optim.zero_grad()\n",
    "            \n",
    "            mask = batch[\"token_mask\"]\n",
    "\n",
    "            res = all_nets[\"ner_net\"].forward(\n",
    "                tokens =         batch[\"token\"],\n",
    "                mask =           mask,\n",
    "                return_mentions=True,\n",
    "                return_loss=False,\n",
    "            )\n",
    "\n",
    "            val_batch = Batcher({\n",
    "                \"mention\": {\n",
    "                    \"mention_id\": torch.arange(n_mentions, n_mentions+len(res['doc_id']), device=device),\n",
    "                    \"begin\": res[\"begin\"],\n",
    "                    \"end\": res[\"end\"],\n",
    "                    \"ner_label\": res[\"ner_label\"],\n",
    "                    \"@sentence_id\": res[\"doc_id\"],\n",
    "                },\n",
    "                \"sentence\": dict(batch[\"sentence\", [\"sentence_id\", \"doc_id\"]]),\n",
    "                \"doc\": dict(batch[\"doc\"])}, \n",
    "                check=False)\n",
    "            \n",
    "            val_batches.append(val_batch)\n",
    "            \n",
    "    # Compute precision, recall and f1 on train set\n",
    "    ner_pred = Batcher.concat(pred_batches)\n",
    "    val_pred = Batcher.concat(val_batches)\n",
    "    \n",
    "    from nlstruct.scoring import compute_metrics, merge_pred_and_gold\n",
    "    \n",
    "    train_metrics = compute_metrics(merge_pred_and_gold(\n",
    "        pred=pd.DataFrame(dict(ner_pred[\"mention\", [\"sentence_id\", \"begin\", \"end\", \"ner_label\"]])), \n",
    "        gold=pd.DataFrame(dict(train_batcher[\"mention\", [\"sentence_id\", \"begin\", \"end\", \"ner_label\"]])), \n",
    "        span_policy='exact',  # only partially match spans with strict bounds, we could also eval with 'exact' or 'partial'\n",
    "        on=[\"sentence_id\", (\"begin\", \"end\"), \"ner_label\"]), prefix='train_')[[\"train_recall\", \"train_precision\", \"train_f1\"]].to_dict()\n",
    "\n",
    "    val_metrics = compute_metrics(merge_pred_and_gold(\n",
    "        pred=pd.DataFrame(dict(val_pred[\"mention\", [\"sentence_id\", \"begin\", \"end\", \"ner_label\"]])), #extract_mentions(val_batcher, all_nets=all_nets)\n",
    "        gold=pd.DataFrame(dict(val_batcher[\"mention\", [\"sentence_id\", \"begin\", \"end\", \"ner_label\"]])), \n",
    "        span_policy='exact',  # only partially match spans with strict bounds, we could also eval with 'exact' or 'partial'\n",
    "        on=[\"sentence_id\", (\"begin\", \"end\"), \"ner_label\"]), prefix='val_')[[\"val_recall\", \"val_precision\", \"val_f1\"]].to_dict()\n",
    "\n",
    "    \n",
    "    # final_schedule.step(val_f1, state[\"epoch\"])\n",
    "\n",
    "    record(\n",
    "    {\n",
    "        \"train_ner_loss\": total_train_ner_loss / max(total_train_ner_size, 1),\n",
    "        **train_metrics,\n",
    "        # **val_metrics,\n",
    "        **val_metrics,\n",
    "        \"n_matched\": n_matched_mentions,\n",
    "        \"lr\": schedules['lr'].get_val()[0],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test \n",
    "(to be fair, avoid executing this part of the notebook to often, or use the training set instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-d3de49966fc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbert_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"bert-large\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;31m#load_genia_ner()#load_from_brat(root.resource(\"deft_2020/t3-test\"), doc_attributes={\"source\": \"real\"})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m test_docs, test_sentences, test_zones, test_mentions, test_conflicts, test_tokens, test_deltas, _ = preprocess(\n\u001b[1;32m      4\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmax_sentence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "bert_name = \"bert-large\"\n",
    "test_dataset=test_dataset#load_genia_ner()#load_from_brat(root.resource(\"deft_2020/t3-test\"), doc_attributes={\"source\": \"real\"})\n",
    "test_docs, test_sentences, test_zones, test_mentions, test_conflicts, test_tokens, test_deltas, _ = preprocess(\n",
    "    dataset=test_dataset,\n",
    "    max_sentence_length=120,\n",
    "    ner_labels=list(vocs[\"ner_label\"]),\n",
    "    bert_name=bert_name,\n",
    "    unknown_labels=\"drop\",\n",
    "    vocabularies=vocs,\n",
    ")\n",
    "test_batcher, test_encoded, test_ids = make_batcher(test_docs, test_sentences, test_zones, test_mentions, test_conflicts, test_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the test mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_label       f1    prec  rec  \n",
      "---------------------------------\n",
      "pathologie      0.420 0.593 0.325\n",
      "sosy            0.524 0.530 0.518\n",
      "---------------------------------\n",
      "total           0.514 0.534 0.496\n"
     ]
    }
   ],
   "source": [
    "pred_batcher = extract_mentions(test_batcher, all_nets=all_nets)\n",
    "gold_batcher = test_batcher\n",
    "\n",
    "pred=pd.DataFrame(dict(pred_batcher[\"mention\", [\"sentence_id\", \"begin\", \"end\", \"ner_label\", \"mention_id\"]]))\n",
    "gold=pd.DataFrame(dict(gold_batcher[\"mention\", [\"@zone_id\", \"begin\", \"end\", \"ner_label\", \"mention_id\"]]))\n",
    "gold[\"sentence_id\"] = gold_batcher[\"zone\", \"sentence_id\"][gold[\"@zone_id\"]]\n",
    "all_preds.append(pred)\n",
    "\n",
    "print(\"{: <15} {: <5} {: <5} {: <5}\".format(\"ner_label\", \"f1\", \"prec\", \"rec\"))\n",
    "print(\"---------------------------------\")\n",
    "for ner_label_idx, ner_label in enumerate(vocs['ner_label']):\n",
    "    merged = merge_pred_and_gold(\n",
    "        pred.query(f'ner_label == {ner_label_idx}'), \n",
    "        gold.query(f'ner_label == {ner_label_idx}'), \n",
    "        span_policy='exact',  # only partially match spans with strict bounds, we could also eval with 'exact' or 'partial'\n",
    "        on=[\"sentence_id\", (\"begin\", \"end\"), \"ner_label\"], atom_gold_level=[\"mention_id\"], atom_pred_level=[\"mention_id\"])\n",
    "    precision = merged['tp'].sum() / merged['pred_count'].sum()\n",
    "    recall = merged['tp'].sum() / merged['gold_count'].sum()\n",
    "    f1 = 2/(1/precision + 1/recall)\n",
    "    f1, precision, recall\n",
    "    print(\"{: <15} {:.3f} {:.3f} {:.3f}\".format(str(ner_label), f1, precision, recall))\n",
    "agg = compute_metrics(merge_pred_and_gold(\n",
    "    pred,\n",
    "    gold,\n",
    "    span_policy='exact',  # only partially match spans with strict bounds, we could also eval with 'exact' or 'partial'\n",
    "    on=[\"sentence_id\", (\"begin\", \"end\"), \"ner_label\"], atom_gold_level=[\"mention_id\"], atom_pred_level=[\"mention_id\"]))\n",
    "print(\"---------------------------------\")\n",
    "print(\"{: <15} {:.3f} {:.3f} {:.3f}\".format(\"total\", agg[\"f1\"], agg[\"precision\"], agg[\"recall\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from nlstruct.text import reverse_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pred_test_batcher = extract_mentions(test_batcher, all_nets=all_nets)\n",
    "df = pd.DataFrame(dict(pred_test_batcher[\"mention\", [\"sentence_id\", \"begin\", \"end\", \"ner_label\", \"mention_id\", \"depth\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pred = (df\n",
    " .merge(\n",
    "    test_encoded[\"tokens\"][[\"sentence_id\", \"token_idx\", \"begin\"]], \n",
    "    left_on=[\"sentence_id\", \"begin\"], right_on=[\"sentence_id\", \"token_idx\"], suffixes=('_token', '')).drop(columns=[\"token_idx\"])\n",
    " .eval(\"end=end-1\")\n",
    " .merge(\n",
    "    test_encoded[\"tokens\"][[\"sentence_id\", \"token_idx\", \"end\"]], \n",
    "    left_on=[\"sentence_id\", \"end\"], right_on=[\"sentence_id\", \"token_idx\"], suffixes=('_token', '')).drop(columns=[\"token_idx\"])\n",
    ").drop(columns=[\"begin_token\", \"end_token\"])\n",
    "pred = pred.merge(test_encoded[\"sentences\"][[\"sentence_id\", \"begin\", \"end\"]], on=[\"sentence_id\"], suffixes=(\"\", \"_sent\")).eval(\n",
    "\"\"\"begin=begin+begin_sent\n",
    "end=end+begin_sent\n",
    "\"\"\").reset_index(drop=True)\n",
    "pred[\"doc_id\"] = test_ids[\"sentence_id\"][\"doc_id\"].iloc[pred[\"sentence_id\"]].reset_index(drop=True)\n",
    "pred[\"ner_label\"] = vocs[\"ner_label\"][pred[\"ner_label\"]]\n",
    "pred = pred.drop(columns=[\"sentence_id\", \"begin_sent\", \"end_sent\"])\n",
    "pred = reverse_deltas(pred, test_deltas, on=[\"doc_id\"])#.query(\"doc_id == 'filehtml-24-cas'\").sort_values(\"begin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 30000)\n",
    "pd.set_option('display.max_columns', 30000)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# x = Prediction\n",
    "x = pred.merge(test_dataset[\"docs\"][[\"doc_id\", \"text\"]])\n",
    "x[\"text_before\"] = x.apply(lambda x: x[\"text\"][x[\"begin\"]-20:x[\"begin\"]], axis=1)\n",
    "x[\"text_after\"] = x.apply(lambda x: x[\"text\"][x[\"end\"]:x[\"end\"]+20], axis=1)\n",
    "x[\"text\"] = x.apply(lambda x: x[\"text\"][x[\"begin\"]:x[\"end\"]], axis=1)\n",
    "x[\"ner_label\"] = x[\"ner_label\"].astype(str)\n",
    "x = x[[\"doc_id\", \"begin\", \"end\", \"text_before\", \"text\", \"text_after\", \"ner_label\", \"depth\"]].rename({\"depth\": \"step\"}, axis=1)\n",
    "x = x.sort_values([\"doc_id\", \"begin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# y = Gold\n",
    "y = test_dataset[\"mentions\"].rename({\"label\": \"ner_label\"}, axis=1).merge(test_dataset[\"fragments\"]).drop(columns=[\"text\"])\n",
    "y = y[y[\"doc_id\"].isin(x[\"doc_id\"])]\n",
    "y = y.merge(test_mentions[[\"doc_id\", \"mention_id\", \"depth\", \"zone_id\"]])\n",
    "y = y.merge(test_dataset[\"docs\"][[\"doc_id\", \"text\"]])\n",
    "y[\"text_before\"] = y.apply(lambda x: x[\"text\"][x[\"begin\"]-20:x[\"begin\"]], axis=1)\n",
    "y[\"text_after\"] = y.apply(lambda x: x[\"text\"][x[\"end\"]:x[\"end\"]+20], axis=1)\n",
    "y[\"text\"] = y.apply(lambda x: x[\"text\"][x[\"begin\"]:x[\"end\"]], axis=1)\n",
    "y[\"ner_label\"] = y[\"ner_label\"].astype(str)\n",
    "y = y[[\"doc_id\", \"begin\", \"end\", \"text_before\", \"text\", \"text_after\", \"ner_label\", \"depth\", \"zone_id\"]]\n",
    "y = y.merge(y[[\"zone_id\", \"depth\"]].eval(\"zone_size = depth\").groupby(\"zone_id\", as_index=False).agg({\"zone_size\": \"max\"}))\n",
    "y = y.sort_values([\"doc_id\", \"begin\"])\n",
    "\n",
    "merged = merge_pred_and_gold(x, y, on=['doc_id', 'begin', 'end', 'ner_label', 'text_before', 'text', 'text_after'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "recall           0.496194\n",
       "precision        0.534676\n",
       "f1               0.514716\n",
       "pred_count    1341.000000\n",
       "gold_count    1445.000000\n",
       "tp             717.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(merged)#.query(\"ner_label.isin(['sosy', 'pathologie'])\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Je ne sais plus trop ce qu'il se passe dans cellules avec w et tout en dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "w = merged_subset.query('tp == 0 and gold_count == 1')[[\"doc_id\", \"begin\", \"end\", \"ner_label\", \"text\"]]\n",
    "rel = factorize_rows(w, merged_subset.query('tp == 1')[['doc_id', 'begin', 'end']], subset=['doc_id', 'begin', 'end'], return_categories=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overlap_size_0</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>begin_pred</th>\n",
       "      <th>end_pred</th>\n",
       "      <th>text_before</th>\n",
       "      <th>text_pred</th>\n",
       "      <th>text_after</th>\n",
       "      <th>ner_label_pred</th>\n",
       "      <th>step</th>\n",
       "      <th>begin_gold</th>\n",
       "      <th>end_gold</th>\n",
       "      <th>ner_label_gold</th>\n",
       "      <th>text_gold</th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gold_count</th>\n",
       "      <th>tp</th>\n",
       "      <th>root</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>filehtml-146-cas</td>\n",
       "      <td>122.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>patiente de 40 ans</td>\n",
       "      <td>épileptique</td>\n",
       "      <td>.\\n\\nLe geste est réal</td>\n",
       "      <td>sosy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>filehtml-146-cas</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>\\nIl n'y a eu aucune</td>\n",
       "      <td>conséquence</td>\n",
       "      <td>pour la patiente en</td>\n",
       "      <td>sosy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>filehtml-146-cas</td>\n",
       "      <td>1174.0</td>\n",
       "      <td>1191.0</td>\n",
       "      <td>nte en dehors d'une</td>\n",
       "      <td>légère somnolence</td>\n",
       "      <td>.\\n</td>\n",
       "      <td>sosy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>filepdf-106-cas</td>\n",
       "      <td>84.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>Elle n'avait pas d'</td>\n",
       "      <td>antécédents pathologiques particuliers</td>\n",
       "      <td>et elle rapportait</td>\n",
       "      <td>sosy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>filepdf-106-cas</td>\n",
       "      <td>160.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>t depuis 6 mois des</td>\n",
       "      <td>douleurs épisodiques localisées au niveau de l’hypochondre droit</td>\n",
       "      <td>sans irradiation et</td>\n",
       "      <td>sosy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overlap_size_0            doc_id  begin_pred  end_pred            text_before                                                         text_pred              text_after ner_label_pred  step  begin_gold  end_gold ner_label_gold text_gold  pred_count  gold_count  tp  root\n",
       "0             NaN  filehtml-146-cas       122.0     133.0    patiente de 40 ans                                                        épileptique  .\\n\\nLe geste est réal           sosy   0.0         NaN       NaN            NaN       NaN           1           0   0     0\n",
       "1             NaN  filehtml-146-cas      1129.0    1140.0  \\nIl n'y a eu aucune                                                        conséquence     pour la patiente en           sosy   0.0         NaN       NaN            NaN       NaN           1           0   0     0\n",
       "2             NaN  filehtml-146-cas      1174.0    1191.0   nte en dehors d'une                                                  légère somnolence                     .\\n           sosy   0.0         NaN       NaN            NaN       NaN           1           0   0     0\n",
       "3             NaN   filepdf-106-cas        84.0     122.0    Elle n'avait pas d'                            antécédents pathologiques particuliers     et elle rapportait            sosy   0.0         NaN       NaN            NaN       NaN           1           0   0     0\n",
       "4             NaN   filepdf-106-cas       160.0     224.0   t depuis 6 mois des   douleurs épisodiques localisées au niveau de l’hypochondre droit     sans irradiation et           sosy   0.0         NaN       NaN            NaN       NaN           1           0   0     0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_pred_and_gold(x, w[rel != -1], on=['doc_id', ('begin', 'end')]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from nlstruct.scoring import compute_confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "merged_span_only = merge_pred_and_gold(x, y, on=['doc_id', 'begin', 'end'])\n",
    "z, labels_str = factorize_rows([merged_span_only.query('tp == 1')[\"ner_label_pred\"], merged_span_only.query('tp == 1')[\"ner_label_gold\"]])\n",
    "labels_str = list(labels_str.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fd5ea2ec950>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 115.2x115.2 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion = compute_confusion_matrix(z[0], z[1], n_labels=len(labels_str))\n",
    "disp = ConfusionMatrixDisplay(confusion, labels_str)\n",
    "_, ax = plt.subplots(1, 1, figsize=(0.8*len(labels_str), 0.8*len(labels_str)))\n",
    "disp.plot(include_values=True, ax=ax, xticks_rotation=\"vertical\", values_format=\".2f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size= 0.0 n_zones= 714\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>step</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depth</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>707</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "step     0  1\n",
       "depth        \n",
       "0      707  7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size= 1.0 n_zones= 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>step</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depth</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "step   0\n",
       "depth   \n",
       "0      1\n",
       "1      2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for zone_size, group in merged.groupby(\"zone_size\"):\n",
    "    u = group.query('tp == 1').astype({\"step\": int, \"depth\": int})\n",
    "    n = len(u.drop_duplicates(\"zone_id\"))\n",
    "    u = u.groupby([\"depth\", \"step\"]).size().unstack(\"step\").fillna(0)\n",
    "    print(\"size=\", zone_size, \"n_zones=\", n)\n",
    "    display(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>step</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depth</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>708.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "step       0    1\n",
       "depth            \n",
       "0      708.0  7.0\n",
       "1        2.0  NaN"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = merged.query('tp == 1')[['step', 'depth', \"ner_label\"]].astype({\"step\": int, \"depth\": int})\n",
    "u = u.groupby([\"depth\", \"step\"]).size().unstack(\"step\")\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<matplotlib.axis.YTick at 0x7fd5ea2eca10>,\n",
       "  <matplotlib.axis.YTick at 0x7fd5e407b510>,\n",
       "  <matplotlib.axis.YTick at 0x7fd56127e150>],\n",
       " [Text(0, 0, '0'), Text(0, 0, '1'), Text(0, 0, '2')],\n",
       " Text(0, 0.5, 'Depth'),\n",
       " [<matplotlib.axis.XTick at 0x7fd5e415a1d0>,\n",
       "  <matplotlib.axis.XTick at 0x7fd5e40cecd0>,\n",
       "  <matplotlib.axis.XTick at 0x7fd56127e1d0>],\n",
       " [Text(0, 0, '0'), Text(0, 0, '1'), Text(0, 0, '2')],\n",
       " Text(0.5, 0, 'Step')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "depths_str = [\"0\", \"1\", \"2\"]\n",
    "steps_str = [\"0\", \"1\", \"2\"]\n",
    "size = 1.5\n",
    "disp = ConfusionMatrixDisplay(u.values, depths_str)\n",
    "_, ax = plt.subplots(1, 1, figsize=(size*len(labels_str), size*len(labels_str)))\n",
    "disp.plot(include_values=True, ax=ax, xticks_rotation=\"horizontal\", values_format=\".2f\", )\n",
    "disp.ax_.set(xticks=np.arange(len(depths_str)),\n",
    "       yticks=np.arange(len(steps_str)),\n",
    "       xticklabels=depths_str,\n",
    "       yticklabels=steps_str,\n",
    "       ylabel=\"Depth\",\n",
    "       xlabel=\"Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "rel = factorize_rows(pred_a, categories=pred_cat, subset=[\"doc_id\", \"begin\", \"end\", \"ner_label\"], return_categories=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pred = (df\n",
    " .merge(\n",
    "    encoded[\"tokens\"][[\"sentence_id\", \"token_idx\", \"begin\"]], \n",
    "    left_on=[\"sentence_id\", \"begin\"], right_on=[\"sentence_id\", \"token_idx\"], suffixes=('_token', ''), how='left').drop(columns=[\"token_idx\"])\n",
    " .eval(\"end=end-1\")\n",
    " .merge(\n",
    "    test_encoded[\"tokens\"][[\"sentence_id\", \"token_idx\", \"end\"]], \n",
    "    left_on=[\"sentence_id\", \"end\"], right_on=[\"sentence_id\", \"token_idx\"], suffixes=('_token', ''), how='left').drop(columns=[\"token_idx\"])\n",
    ").drop(columns=[\"begin_token\", \"end_token\"])\n",
    "pred = pred.merge(test_encoded[\"sentences\"][[\"sentence_id\", \"begin\", \"end\"]], on=[\"sentence_id\"], suffixes=(\"\", \"_sent\")).eval(\n",
    "\"\"\"begin=begin+begin_sent\n",
    "end=end+begin_sent\n",
    "\"\"\").reset_index(drop=True)\n",
    "pred[\"doc_id\"] = test_ids[\"sentence_id\"][\"doc_id\"].iloc[pred[\"sentence_id\"]].reset_index(drop=True)\n",
    "pred[\"ner_label\"] = vocs[\"ner_label\"][pred[\"ner_label\"]]\n",
    "pred = pred.drop(columns=[\"sentence_id\", \"begin_sent\", \"end_sent\"])\n",
    "pred = reverse_deltas(pred, deltas, on=[\"doc_id\"])#.query(\"doc_id == 'filehtml-24-cas'\").sort_values(\"begin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ner_label</th>\n",
       "      <th>mention_id</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_before</th>\n",
       "      <th>text_after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>traitement</td>\n",
       "      <td>1650</td>\n",
       "      <td>1307</td>\n",
       "      <td>1325</td>\n",
       "      <td>filepdf-839-cas</td>\n",
       "      <td>ouverture du kyste</td>\n",
       "      <td>tectomie en bloc. L’</td>\n",
       "      <td>donne issue à un li</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>anatomie</td>\n",
       "      <td>1651</td>\n",
       "      <td>1451</td>\n",
       "      <td>1469</td>\n",
       "      <td>filepdf-839-cas</td>\n",
       "      <td>hydatique mammaire</td>\n",
       "      <td>imprévu d’un kyste</td>\n",
       "      <td>.\\n\\nL’étude histologi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>pathologie</td>\n",
       "      <td>1718</td>\n",
       "      <td>1445</td>\n",
       "      <td>1469</td>\n",
       "      <td>filepdf-839-cas</td>\n",
       "      <td>kyste hydatique mammaire</td>\n",
       "      <td>stique imprévu d’un</td>\n",
       "      <td>.\\n\\nL’étude histologi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>examen</td>\n",
       "      <td>3034</td>\n",
       "      <td>1474</td>\n",
       "      <td>1492</td>\n",
       "      <td>filepdf-839-cas</td>\n",
       "      <td>étude histologique</td>\n",
       "      <td>atique mammaire.\\n\\nL’</td>\n",
       "      <td>du prélèvement préc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>examen</td>\n",
       "      <td>3035</td>\n",
       "      <td>1496</td>\n",
       "      <td>1507</td>\n",
       "      <td>filepdf-839-cas</td>\n",
       "      <td>prélèvement</td>\n",
       "      <td>ude histologique du</td>\n",
       "      <td>précise que le kyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>anatomie</td>\n",
       "      <td>3036</td>\n",
       "      <td>1547</td>\n",
       "      <td>1556</td>\n",
       "      <td>filepdf-839-cas</td>\n",
       "      <td>membranes</td>\n",
       "      <td>e est formé par des</td>\n",
       "      <td>faiblement acidophi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>sosy</td>\n",
       "      <td>3108</td>\n",
       "      <td>1523</td>\n",
       "      <td>1591</td>\n",
       "      <td>filepdf-839-cas</td>\n",
       "      <td>kyste est formé par des membranes faiblement acidophiles feuilletées</td>\n",
       "      <td>ment précise que le</td>\n",
       "      <td>avec la présence de</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ner_label  mention_id  begin   end           doc_id                                                                  text             text_before              text_after\n",
       "398  traitement        1650   1307  1325  filepdf-839-cas                                                    ouverture du kyste    tectomie en bloc. L’     donne issue à un li\n",
       "399    anatomie        1651   1451  1469  filepdf-839-cas                                                    hydatique mammaire     imprévu d’un kyste   .\\n\\nL’étude histologi\n",
       "400  pathologie        1718   1445  1469  filepdf-839-cas                                              kyste hydatique mammaire    stique imprévu d’un   .\\n\\nL’étude histologi\n",
       "416      examen        3034   1474  1492  filepdf-839-cas                                                    étude histologique  atique mammaire.\\n\\nL’     du prélèvement préc\n",
       "417      examen        3035   1496  1507  filepdf-839-cas                                                           prélèvement    ude histologique du      précise que le kyst\n",
       "418    anatomie        3036   1547  1556  filepdf-839-cas                                                             membranes    e est formé par des      faiblement acidophi\n",
       "423        sosy        3108   1523  1591  filepdf-839-cas  kyste est formé par des membranes faiblement acidophiles feuilletées    ment précise que le      avec la présence de"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pred_a.merge(test_dataset[\"docs\"][[\"doc_id\", \"text\"]])\n",
    "y[\"text_before\"] = y.apply(lambda x: x[\"text\"][x[\"begin\"]-20:x[\"begin\"]], axis=1)\n",
    "y[\"text_after\"] = y.apply(lambda x: x[\"text\"][x[\"end\"]:x[\"end\"]+20], axis=1)\n",
    "y[\"text\"] = y.apply(lambda x: x[\"text\"][x[\"begin\"]:x[\"end\"]], axis=1)\n",
    "y.query(\"doc_id == 'filepdf-839-cas' and begin > 1300 and end < 1600\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ner_label</th>\n",
       "      <th>mention_id</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_before</th>\n",
       "      <th>text_after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>traitement</td>\n",
       "      <td>1652</td>\n",
       "      <td>1307</td>\n",
       "      <td>1316</td>\n",
       "      <td>filepdf-839-cas</td>\n",
       "      <td>ouverture</td>\n",
       "      <td>tectomie en bloc. L’</td>\n",
       "      <td>du kyste donne issu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>anatomie</td>\n",
       "      <td>1653</td>\n",
       "      <td>1451</td>\n",
       "      <td>1469</td>\n",
       "      <td>filepdf-839-cas</td>\n",
       "      <td>hydatique mammaire</td>\n",
       "      <td>imprévu d’un kyste</td>\n",
       "      <td>.\\n\\nL’étude histologi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>sosy</td>\n",
       "      <td>1722</td>\n",
       "      <td>1445</td>\n",
       "      <td>1469</td>\n",
       "      <td>filepdf-839-cas</td>\n",
       "      <td>kyste hydatique mammaire</td>\n",
       "      <td>stique imprévu d’un</td>\n",
       "      <td>.\\n\\nL’étude histologi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>examen</td>\n",
       "      <td>3021</td>\n",
       "      <td>1474</td>\n",
       "      <td>1492</td>\n",
       "      <td>filepdf-839-cas</td>\n",
       "      <td>étude histologique</td>\n",
       "      <td>atique mammaire.\\n\\nL’</td>\n",
       "      <td>du prélèvement préc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>examen</td>\n",
       "      <td>3022</td>\n",
       "      <td>1496</td>\n",
       "      <td>1507</td>\n",
       "      <td>filepdf-839-cas</td>\n",
       "      <td>prélèvement</td>\n",
       "      <td>ude histologique du</td>\n",
       "      <td>précise que le kyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>sosy</td>\n",
       "      <td>3023</td>\n",
       "      <td>1523</td>\n",
       "      <td>1591</td>\n",
       "      <td>filepdf-839-cas</td>\n",
       "      <td>kyste est formé par des membranes faiblement acidophiles feuilletées</td>\n",
       "      <td>ment précise que le</td>\n",
       "      <td>avec la présence de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>anatomie</td>\n",
       "      <td>3090</td>\n",
       "      <td>1547</td>\n",
       "      <td>1556</td>\n",
       "      <td>filepdf-839-cas</td>\n",
       "      <td>membranes</td>\n",
       "      <td>e est formé par des</td>\n",
       "      <td>faiblement acidophi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ner_label  mention_id  begin   end           doc_id                                                                  text             text_before              text_after\n",
       "401  traitement        1652   1307  1316  filepdf-839-cas                                                             ouverture    tectomie en bloc. L’     du kyste donne issu\n",
       "402    anatomie        1653   1451  1469  filepdf-839-cas                                                    hydatique mammaire     imprévu d’un kyste   .\\n\\nL’étude histologi\n",
       "403        sosy        1722   1445  1469  filepdf-839-cas                                              kyste hydatique mammaire    stique imprévu d’un   .\\n\\nL’étude histologi\n",
       "419      examen        3021   1474  1492  filepdf-839-cas                                                    étude histologique  atique mammaire.\\n\\nL’     du prélèvement préc\n",
       "420      examen        3022   1496  1507  filepdf-839-cas                                                           prélèvement    ude histologique du      précise que le kyst\n",
       "421        sosy        3023   1523  1591  filepdf-839-cas  kyste est formé par des membranes faiblement acidophiles feuilletées    ment précise que le      avec la présence de\n",
       "425    anatomie        3090   1547  1556  filepdf-839-cas                                                             membranes    e est formé par des      faiblement acidophi"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pred_b.merge(test_dataset[\"docs\"][[\"doc_id\", \"text\"]])\n",
    "y[\"text_before\"] = y.apply(lambda x: x[\"text\"][x[\"begin\"]-20:x[\"begin\"]], axis=1)\n",
    "y[\"text_after\"] = y.apply(lambda x: x[\"text\"][x[\"end\"]:x[\"end\"]+20], axis=1)\n",
    "y[\"text\"] = y.apply(lambda x: x[\"text\"][x[\"begin\"]:x[\"end\"]], axis=1)\n",
    "y.query(\"doc_id == 'filepdf-839-cas' and begin > 1300 and end < 1600\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ner_label</th>\n",
       "      <th>mention_id</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_before</th>\n",
       "      <th>text_after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>traitement</td>\n",
       "      <td>1664</td>\n",
       "      <td>1307</td>\n",
       "      <td>1325</td>\n",
       "      <td>filepdf-839-cas</td>\n",
       "      <td>ouverture du kyste</td>\n",
       "      <td>tectomie en bloc. L’</td>\n",
       "      <td>donne issue à un li</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>anatomie</td>\n",
       "      <td>1665</td>\n",
       "      <td>1451</td>\n",
       "      <td>1469</td>\n",
       "      <td>filepdf-839-cas</td>\n",
       "      <td>hydatique mammaire</td>\n",
       "      <td>imprévu d’un kyste</td>\n",
       "      <td>.\\n\\nL’étude histologi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>sosy</td>\n",
       "      <td>1734</td>\n",
       "      <td>1445</td>\n",
       "      <td>1469</td>\n",
       "      <td>filepdf-839-cas</td>\n",
       "      <td>kyste hydatique mammaire</td>\n",
       "      <td>stique imprévu d’un</td>\n",
       "      <td>.\\n\\nL’étude histologi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>examen</td>\n",
       "      <td>3044</td>\n",
       "      <td>1474</td>\n",
       "      <td>1492</td>\n",
       "      <td>filepdf-839-cas</td>\n",
       "      <td>étude histologique</td>\n",
       "      <td>atique mammaire.\\n\\nL’</td>\n",
       "      <td>du prélèvement préc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>examen</td>\n",
       "      <td>3045</td>\n",
       "      <td>1496</td>\n",
       "      <td>1507</td>\n",
       "      <td>filepdf-839-cas</td>\n",
       "      <td>prélèvement</td>\n",
       "      <td>ude histologique du</td>\n",
       "      <td>précise que le kyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>anatomie</td>\n",
       "      <td>3046</td>\n",
       "      <td>1547</td>\n",
       "      <td>1556</td>\n",
       "      <td>filepdf-839-cas</td>\n",
       "      <td>membranes</td>\n",
       "      <td>e est formé par des</td>\n",
       "      <td>faiblement acidophi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ner_label  mention_id  begin   end           doc_id                      text             text_before              text_after\n",
       "403  traitement        1664   1307  1325  filepdf-839-cas        ouverture du kyste    tectomie en bloc. L’     donne issue à un li\n",
       "404    anatomie        1665   1451  1469  filepdf-839-cas        hydatique mammaire     imprévu d’un kyste   .\\n\\nL’étude histologi\n",
       "405        sosy        1734   1445  1469  filepdf-839-cas  kyste hydatique mammaire    stique imprévu d’un   .\\n\\nL’étude histologi\n",
       "421      examen        3044   1474  1492  filepdf-839-cas        étude histologique  atique mammaire.\\n\\nL’     du prélèvement préc\n",
       "422      examen        3045   1496  1507  filepdf-839-cas               prélèvement    ude histologique du      précise que le kyst\n",
       "423    anatomie        3046   1547  1556  filepdf-839-cas                 membranes    e est formé par des      faiblement acidophi"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pred_c.merge(test_dataset[\"docs\"][[\"doc_id\", \"text\"]])\n",
    "y[\"text_before\"] = y.apply(lambda x: x[\"text\"][x[\"begin\"]-20:x[\"begin\"]], axis=1)\n",
    "y[\"text_after\"] = y.apply(lambda x: x[\"text\"][x[\"end\"]:x[\"end\"]+20], axis=1)\n",
    "y[\"text\"] = y.apply(lambda x: x[\"text\"][x[\"begin\"]:x[\"end\"]], axis=1)\n",
    "y.query(\"doc_id == 'filepdf-839-cas' and begin > 1300 and end < 1600\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ner_label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4410\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4411\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlibindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value_at\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4412\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.get_value_at\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.get_value_at\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/util.pxd\u001b[0m in \u001b[0;36mpandas._libs.util.get_value_at\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/util.pxd\u001b[0m in \u001b[0;36mpandas._libs.util.validate_indexer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-dad1791feb01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m     ),\n\u001b[1;32m     56\u001b[0m     \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/home/yoann/these/DEFT/preds/genia\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mexport_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-39-dad1791feb01>\u001b[0m in \u001b[0;36mexport_mentions_to_brat\u001b[0;34m(dataset, path, export_text, raise_empty_mentions)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"begin\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mspans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0mner_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ner_label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ner_label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4417\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4418\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4419\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4420\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4421\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4403\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"getitem\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4404\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4405\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4406\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4407\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ner_label'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def export_mentions_to_brat(dataset, path=None, export_text=False, raise_empty_mentions=True):\n",
    "    doc_id_to_text = dict(zip(dataset[\"docs\"][\"doc_id\"], dataset[\"docs\"][\"text\"]))\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    for doc_id, mentions in dataset[\"mentions\"].groupby(\"doc_id\"):\n",
    "        if export_text:\n",
    "            f = None\n",
    "            try:\n",
    "                if path is not None:\n",
    "                    f = open(\"{}/{}.text\".format(path, doc_id), \"w\")\n",
    "                else:\n",
    "                    print(\"File :\", \"{}.txt\".format(doc_id))\n",
    "                print(doc_id_to_text[doc_id], file=f)\n",
    "            finally:\n",
    "                f.close()\n",
    "        f = None\n",
    "        try:\n",
    "            if path is not None:\n",
    "                f = open(\"{}/{}.ann\".format(path, doc_id), \"w\")\n",
    "            else:\n",
    "                print(\"File :\", \"{}.ann\".format(doc_id))\n",
    "            \n",
    "            mention_counter = 0\n",
    "            \n",
    "            for mention_i, (_, row) in enumerate(mentions.sort_values(\"begin\").iterrows()):\n",
    "                mention_counter += 1\n",
    "                text = doc_id_to_text[doc_id][row[\"begin\"]:row[\"end\"]]\n",
    "                idx = row[\"begin\"]\n",
    "                spans = []\n",
    "                ner_label = str(row[\"ner_label\"])\n",
    "                if \"\\n\" in text:\n",
    "                    print(repr(text), row[\"ner_label\"])\n",
    "                for part in text.split(\"\\n\"):\n",
    "                    begin = idx\n",
    "                    end = idx + len(part)\n",
    "                    idx = end + 1\n",
    "                    if begin != end:\n",
    "                        spans.append((begin, end))\n",
    "                    elif raise_empty_mentions:\n",
    "                        raise Exception(f\"Empty mention in {doc_id}: {begin}-{end} ({ner_label})\")\n",
    "                print(\"T{}\\t{} {}\\t{}\".format(mention_i+1, ner_label, \";\".join(\" \".join(map(str, span)) for span in spans), text.replace(\"\\n\", \" \")), file=f)\n",
    "        finally:\n",
    "            if f is not None:\n",
    "                f.close()\n",
    "\n",
    "#pred_path = \"/home/shared/resources/deft_2020/equipe-LIMICS-tache3-run-2\" # 999b3b047bce084f\n",
    "export_mentions_to_brat(\n",
    "    Dataset(\n",
    "        docs=test_dataset[\"docs\"],\n",
    "        mentions=test_dataset[\"fragments\"]\n",
    "    ),\n",
    "    path=\"/home/yoann/these/DEFT/preds/genia\",\n",
    "    export_text=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_mentions = test_dataset[\"mentions\"].merge(test_dataset[\"fragments\"], on=[\"doc_id\", \"mention_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "def preds_to_ann(\n",
    "    mentions,\n",
    "    dataset,\n",
    "    ann_path,\n",
    "):\n",
    "    \n",
    "    from pathlib import Path\n",
    "    Path(ann_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for doc_id in dataset['docs']['doc_id']:\n",
    "        doc_mentions = mentions[mentions['doc_id']==doc_id]\n",
    "        \n",
    "        m_list = [f'T{i}\\t{m.label} {m.begin} {m.end}\\t{m.text}'\\\n",
    "        for i, (_,m) in enumerate(doc_mentions[~doc_mentions['text'].str.contains('\\n')].iterrows())]\n",
    "        ann_str = '\\n'.join(m_list)\n",
    "        \n",
    "        txt_str = dataset['docs'][dataset['docs']['doc_id']==doc_id].text.tolist()[0]\n",
    "        \n",
    "        # Handle linebreaks in predictions\n",
    "        # For now by separating into several entities\n",
    "        if sum(doc_mentions['text'].str.contains('\\n')) > 0:\n",
    "            i = len(m_list)\n",
    "            j = 0\n",
    "            \n",
    "            for _,m in doc_mentions[doc_mentions['text'].str.contains('\\n')].iterrows():\n",
    "                lines = m.text.split('\\n')\n",
    "                begin = m.begin\n",
    "                span_str = f'{begin}'\n",
    "                char_skip = 0\n",
    "                for l in lines:\n",
    "                    if l != '':\n",
    "                        begin += char_skip\n",
    "                        end = begin + len(l)\n",
    "                        ann_str += f'\\nT{i+j}\\t{m.label} {begin} {end}\\t{l}'\n",
    "                        begin = end + 1\n",
    "                        j += 1\n",
    "                        char_skip = 0\n",
    "                    else:\n",
    "                        char_skip += 1\n",
    "        \n",
    "        with open(path.join(ann_path, f'{doc_id}.a2'), 'w') as f:\n",
    "            f.write(ann_str)\n",
    "            \n",
    "#         with open(path.join(ann_path, f'{doc_id}.txt'), 'w') as f:\n",
    "#             f.write(txt_str)\n",
    "            \n",
    "from datetime import datetime\n",
    "now = datetime.now().strftime('%d%m_%H%M%S')\n",
    "\n",
    "prediction_path = f'/home/yoann/these/DEFT/preds/genia/{now}'\n",
    "                \n",
    "preds_to_ann(gold_mentions, test_dataset, prediction_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yt_nlp",
   "language": "python",
   "name": "yt_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
